这一次放出来的职位包括几个方面：
大模型推理方向
工作范围
Build领域STA的大模型推理服务系统，涉及到的技术范围涵盖大模型推理系统的全栈，包括模型推理优化、serving系统优化，量化压缩等。
希望的候选人背景
对经典大模型wrklad的计算访存行为有着清晰深入的认识，比如能够根据模型结构spec及sequence length大体推断出来来一条计算样本对应的计算量和理论显存需求。
熟悉大模型推理场景的E2E链路细节，包括但不限于各种sampling算法实现，高并发在线服务，向量数据库等。
对大模型推理优化技术栈有一定的理解和认识，比如K-V cache优化，Fused Multi-head-attentin优化，等等。
对TensrRT，FasterTransfrmer等NV主流推理优化工具有比较深入的理解和认识。
扎实的系统工程能力，包括但不限于C++，Pythn工程实现能力。
扎实的计算机体系结构背景。
有过复杂软件系统开发经验者优先。
AI编译方向
工作范围
Build领域STA的AI编译系统，重点面向计算密集算子代码生成。
希望的候选人背景
对AI编译主流技术栈有比较深入的理解和认识，包括但不限于MLIR、TVM，XLA等。
有过AI编译系统直接开发经验者优先。
对NV GPU体系架构有扎实的理解。
扎实的系统工程能力，包括但不限于C++，Pythn工程实现能力。
感兴趣的同学可以邮件juney@nvidia.cm
注：这两个职位同样有intern机会可选。

~~~
哈哈标题党了，我其实不是学生，分享一下我作为一名面试官常问的AI推理加速HPC岗位的问题~仅分享通用知识部分，项目部分因人而异这里省略

C++部分

1、为什么我们做C++项目的时候，需要写头文件？

2、讲出static关键字的一种应用场景

3、单例模式如何实现？

4、讲讲四种类型转换

5、拷贝构造函数中浅拷贝和深拷贝的区别？

6、一个类要去访问另一个类的private数据成员，该如何操作？

量化quantization部分

1、说说你知道的那些针对LLM的量化技法？

2、smoothquant为什么可以解决int8 LLM的accuracy问题？

3、bfloat16和fp16(half float point)同样内存大小，那么它们可以节约的内存大小应该是一样的吗？他们的优缺点主要有哪些？

4、量化怎么平衡精度和速度？

CUDA部分

1、讲讲shared memory bank conflict的发生场景？以及你能想到哪些解决方案？

2、CUDA里面如何分配GPU显存？为什么里面的参数是二级指针？

3、优化CUDA程序的访存效率，你可以想到哪些？

4、优化CUDA程序的计算效率，你又可以想到哪些？

大模型部分

1、有哪些encoder-only、decoder-only、encoder-decoder的模型？

2、随着seqlen的增加，你觉得encoder-only的模型和decoder-only的模型的计算量和访存量会是哪些变化趋势？为什么？

3、说说你知道的大模型训练or推理的常用优化手段

4、一般会对哪些大模型里面的算子做算子融合，说说你知道的

5、flash attention的原理讲讲？你认为为什么flash attention极大提升了训练速度？

6、paged attention的原理讲讲？你认为为什么paged attention极大提升了推理速度？它和flash attention的区别是什么？

以上呢，起始都是非常经典的问题，足够看出大家对AI推理加速HPC岗位的基础。

作者：nino17
链接：https://www.nowcoder.com/discuss/594109075609174016
来源：牛客网
~~~