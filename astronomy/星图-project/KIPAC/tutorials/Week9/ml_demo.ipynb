{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reminder!\n",
    "\n",
    "After pulling down the tutorial notebook, immediately make a copy. Then do not modify the original. Do your work in the copy. This will prevent the possibility of git conflicts should the version-controlled file change at any point in the future. (The same exhortation applies to homeworks.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 9 Tutorial \n",
    "\n",
    "## Introduction to Machine Learning\n",
    "\n",
    "In this 3-part notebook you will start to explore the `scikit-learn` ML python package, and see how it supports a range of machine learning models with a uniform terminology and API, and emphasize model evaluation by cross-validation. \n",
    "\n",
    "> Credit: some of the material in this tutorial is based on Andy Mueller's `scikit-learn` tutorial from, the 2015 edition of \"Astro Hack Week\". The SDSS examples are based on Josh Bloom's SDSS example notebook, from the 2014 edition of \"Astro Hack Week\".\n",
    "\n",
    "> Further reading: Ivezic Chapter 8 (regression) and Chapter 9 (classification).\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to `pip install scikit-learn` and check that you have v0.18 or higher as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Simple Example: The Digits Dataset\n",
    "\n",
    "* Let's take a look at one of the `SciKit-Learn` example datasets, `digits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(digits.images[23], cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.target[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* In `SciKit-Learn`,  `data` contains the design matrix $X$, and is a `numpy` array of shape $(N, P)$\n",
    "\n",
    "\n",
    "* `target` contains the response variables $y$, and is a `numpy` array of shape $(N)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(digits.data, digits.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Example Datasets\n",
    "\n",
    "`SciKit-Learn` provides 5 \"toy\" datasets for tutorial purposes, all `load`-able in the same way:\n",
    "\n",
    "Name        | Description\n",
    "------------|:---------------------------------------\n",
    "`boston`\t| Boston house-prices, with 13 associated measurements (R)\n",
    "`iris`\t    | Fisher's iris classifications (based on 4 characteristics) (C)\n",
    "`diabetes`\t| Diabetes (x vs y) (R)\n",
    "`digits`\t| Hand-written digits, 8x8 images with classifications (C)\n",
    "`linnerud`\t| Linnerud: 3 exercise and 3 physiological data (R)\n",
    "\n",
    "\n",
    "* \"R\" and \"C\" indicate that the problem to be solved is either a regression or a classification, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Looking for Structure\n",
    "\n",
    "* A model's ability to make predictions depends on there being _structure_ in the data\n",
    "\n",
    "* If structure is present the data are informative, and vice versa.\n",
    "\n",
    "* Feature design takes thought; thinking is aided by _data visualization_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing the Boston house price data:\n",
    "\n",
    "import corner\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "plot = np.concatenate((X, np.atleast_2d(y).T), axis=1)\n",
    "labels = np.append(boston.feature_names,'MEDV')\n",
    "\n",
    "corner.corner(plot, labels=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Fitting a Straight Line with `SciKit-Learn`\n",
    "\n",
    "For a first application, let's see the straight line fitting problem solved via the machine learning approach. In the process, we'll look at how model (prediction) accuracy is quantified, and then generalized via cross-validation.  \n",
    "\n",
    "### Further Reading\n",
    "\n",
    "Ivezic Sections 8.1 and 8.2 (linear regression), and Section 8.11 for cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression\n",
    "\n",
    "Straight line fitting is a [linear regression](http://scikit-learn.org/stable/modules/linear_model.html) problem - and an example of predictive learning. \n",
    "\n",
    "A predictive model can be said to have been \"fitted\" to the data when an assumed _loss function_ has been minimized. A popular choice of minimized loss function is the following, corresponding to the method of \"ordinary least squares\":\n",
    "\n",
    "$$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2 $$\n",
    "\n",
    "> While this loss function is derivable from statistical principles, the machine only needs it to be encoded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "If we fit a straight line to a subset of the data (the training set), the accuracy of the linear model's predictions in the remainder of the data (the test set) can be checked. \n",
    "\n",
    "Let's fit some test data with a straight line using the `SciKit-Learn` library, and see how accurately we can make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Code source: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Load the boston dataset, and focus on just one attribute: \n",
    "# LSTAT (attribute 12)\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "# Package into design matrix X and target vector y:\n",
    "X = np.atleast_2d(boston.data[:,12]).T\n",
    "y = np.atleast_2d(boston.target).T\n",
    "\n",
    "# Make a training/test split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test, color='black')\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The `Linear` Model\n",
    "\n",
    "`scikit-learn` machine learning models have a common API:\n",
    "\n",
    "* a `fit` method that optimizes the model's internal parameters given training data features and target values\n",
    "\n",
    "* a `predict` method that returns model-predicted target values given test data features\n",
    "\n",
    "* various `score`s for quantifying performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create linear regression model object:\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training set:\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# The coefficients:\n",
    "print(\"Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scoring\n",
    "\n",
    "* The \"mean squared error\" between the model predictions and the truth is a useful metric: minimizing MSE corresponds to minimizing the \"empirical risk,\" defined as the mean value loss function averaged over the available data samples, where the loss function is quadratic:\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;{\\rm MSE} = \\mathcal{E} \\left[ (\\hat{y} - y^{\\rm true})^2 \\right] = \\mathcal{E} \\left[ (\\hat{y} - \\bar{y} + \\bar{y} - y^{\\rm true})^2 \\right] = \\mathcal{E} \\left[ (\\hat{y} - \\bar{y})^2 \\right] + (\\bar{y} - y^{\\rm true})^2$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = {\\rm var}(\\hat{y}) + {\\rm bias}^2(\\hat{y})$\n",
    "\n",
    "\n",
    "* In general, different models reach different balances between the variance and bias of their predictions\n",
    "\n",
    "* A particular choice of loss function leads to a corresponding minimized risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The mean square prediction error:\n",
    "print(\"Training data: MSE = %.2f\"\n",
    "      % np.mean((model.predict(X_train) - y_train) ** 2))\n",
    "print(\"Test data: MSE = %.2f\"\n",
    "      % np.mean((model.predict(X_test) - y_test) ** 2))\n",
    "\n",
    "# The \"explained variance\" R2 score: 1 is perfect prediction:\n",
    "print('Training data: R^2 score = %.2f' % model.score(X_train, y_train))\n",
    "print('Test data: R^2 score = %.2f' % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### R2 scores\n",
    "\n",
    "* The \"explained variance\" $R^2$ \"score\" is defined as $(1 - u/v)$, where $u$ is the *regression sum of squares* $\\sum (y_{\\rm true} - y_{\\rm pred})^2$ and $v$ is the *residual sum of squares* $\\sum (y_{\\rm true} - \\overline{y_{\\rm true}})^2)$. \n",
    "\n",
    "\n",
    "* The best possible $R^2$ score is 1.0. A model that has mean squared error equal to the variance in the data gets a score of zero. Models that do systematically worse than this have negative $R^2$ scores.\n",
    "\n",
    "\n",
    "* In general we expect the training score to be higher than the test score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot outputs:\n",
    "plt.scatter(X_test, y_test,  color='black')\n",
    "plt.plot(X_test, model.predict(X_test), color='blue', linewidth=3)\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Questions:\n",
    "\n",
    "* How is this procedure different from previous occasions we have fitted a straight line? \n",
    "\n",
    "* Is it \"Frequentist\" or \"Bayesian\"? Why? \n",
    "\n",
    "* Does it follow the likelihood principle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizing Model Prediction Accuracy\n",
    "\n",
    "* In supervised machine learning the usual goal is to make the most accurate predictions we can - which means neither over-fitting nor under-fitting the data \n",
    "\n",
    "* Above, we made one training/test split, and computed the (mean squared) prediction error. The model that minimizes the *generalized prediction error* can be found (approximately) with *cross validation*.\n",
    "\n",
    "* In cross validation we consider multiple training/test splits, and look at the _mean score_ across all of these _\"folds.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "cross_val_score(model, X, y, cv=5, scoring='r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Validation Fold Design\n",
    "\n",
    "* How we design the folds matters: we want each subset of the data to be a _fair sample_ of the whole.\n",
    "\n",
    "\n",
    "* In this problem, we want to select the LSTAT values randomly (rather than sequentially), and so we make a `ShuffleSplit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "shuffle_split = ShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "cross_val_score(model, X, y, cv=shuffle_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized Scoring\n",
    "\n",
    "With our 10 fold shuffle splits, we can calculate generalized accuracy scores  - that could be used in a cross-validation model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = cross_val_score(model, X, y, cv=shuffle_split, scoring='neg_mean_squared_error')\n",
    "GE, errGE = -np.mean(MSE), np.std(MSE)/np.sqrt(len(MSE))\n",
    "print(\"Generalization error:\", GE, \"+/-\", errGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = cross_val_score(model, X, y, cv=shuffle_split, scoring='r2')\n",
    "meanR2, errR2 = np.mean(R2), np.std(R2)/np.sqrt(len(R2))\n",
    "print(\"Generalized R2 score:\", meanR2, \"+/-\", errR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Expansion\n",
    "\n",
    "* Let's expand our linear model to include some higher order terms (quadratic, cubic etc). This can be done by adding additional feature columns to the design matrix $X$. We are still just predicting $y$, but now we'll be asking for more coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "XX = poly.fit_transform(X)\n",
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polymodel = linear_model.LinearRegression()\n",
    "\n",
    "poly_split = ShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "\n",
    "R2 = cross_val_score(polymodel, XX, y, cv=poly_split, scoring='r2')\n",
    "\n",
    "poly_meanR2, poly_errR2 = np.mean(R2), np.std(R2)/np.sqrt(len(R2))\n",
    "print(\"Polynomial: generalized R^2 score:\", np.round(poly_meanR2, 2), \"+/-\", np.round(poly_errR2, 2))\n",
    "print(\"Straight line: generalized R^2 score:\", np.round(meanR2, 2), \"+/-\", np.round(errR2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Checking\n",
    "\n",
    "As usual, it's a good idea to check the model's performance by visualizing its predictions in data space\n",
    "\n",
    "\n",
    "We can make one model prediction for each training set in a set of folds, and plot all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "\n",
    "y_straightline = cross_val_predict(model, X, y, cv=10)\n",
    "y_polynomial = cross_val_predict(polymodel, XX, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot outputs:\n",
    "plt.scatter(X, y,  color='black', alpha=0.1)\n",
    "plt.plot(X, y_straightline, color='blue', linewidth=2, alpha=0.4)\n",
    "plt.plot(X, y_polynomial, color='green', linewidth=2, alpha=0.4)\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In this example, the polynomial degree is a control parameter that needs to be set: we can search this parameter space for the value that gives the highest average cross validation score (or lowest generalization error). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "* The Boston dataset has 13 attributes, more than one of which might contain information about house prices in the city. Let's train a linear model on all these attributes, and see if we can improve our score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear model:\n",
    "supermodel = linear_model.LinearRegression()\n",
    "\n",
    "# Use all the data, and set up a 10-fold cross validation run:\n",
    "super_split = ShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "\n",
    "# Carry out the cross-validation of the model, training, testing and reporting:\n",
    "R2 = cross_val_score(supermodel, boston.data, boston.target, cv=super_split, scoring='r2')\n",
    "                           \n",
    "# Compute our model prediction accuracy score, for comparison with other models:\n",
    "hpmeanR2, hperrR2 = np.mean(R2), np.std(R2)/np.sqrt(len(R2))\n",
    "print(\"Hyperplane: generalized R^2 score:\", np.round(hpmeanR2, 2), \"+/-\", np.round(hperrR2, 2))\n",
    "print(\"Straight line: generalized R^2 score:\", np.round(meanR2, 2), \"+/-\", np.round(errR2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What just happened?\n",
    "\n",
    "* We just went from a simple hypothesis (median house price `MEDV` depends on `LSTAT`) to a very much more complex one (house price could depend on all of our 13 measured attributes) in one step. The data analysis is *automated*, in the sense that we simply fed our machine new inputs and it processed them.\n",
    "\n",
    "\n",
    "* Using all our data, we are now better at predicting house prices - *but we have gained no new understanding of how the Boston housing market works.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Machine Learning with the SDSS\n",
    "\n",
    "Now let's look at a couple of astronomical applications of machine learning, an example regression and classification analysis of the SDSS catalog dataset. \n",
    "\n",
    "In the process, we'll understand some more \"score\" metrics and diagnostic visualizations, and carry out model selection by cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Photometric Redshift Estimation and Quasar Classification\n",
    "\n",
    "Let's investigate the SDSS photometric object catalog, and look for machine learning solutions to the following two problems:\n",
    "\n",
    "1. Estimating the redshifts of quasars from their photometry (regression)\n",
    "\n",
    "2. Selecting quasars from a background of stars and galaxies (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Aquisition\n",
    "\n",
    "From the SDSS Sky Server we've downloaded two types of photometry (aperature and petrosian), corrected for extinction, for a number of sources with redshifts. Here's the SQL for an example query, that gets us 10000 example quasars:\n",
    "\n",
    "<font color=\"blue\" size=2>\n",
    "<pre>SELECT *,dered_u - mag_u AS diff_u, dered_g - mag_g AS diff_g, dered_r - mag_r AS diff_g, dered_i - mag_i AS diff_i, dered_z - mag_z AS diff_z from\n",
    "(SELECT top 10000\n",
    "objid, ra, dec, dered_u,dered_g,dered_r,dered_i,dered_z,psfmag_u-extinction_u AS mag_u,\n",
    "psfmag_g-extinction_g AS mag_g, psfmag_r-extinction_r AS mag_r, psfmag_i-extinction_i AS mag_i,psfmag_z-extinction_z AS mag_z,z AS spec_z,dered_u - dered_g AS u_g_color, \n",
    "dered_g - dered_r AS g_r_color,dered_r - dered_i AS r_i_color,dered_i - dered_z AS i_z_color,class\n",
    "FROM SpecPhoto \n",
    "WHERE (class = 'QSO') ) as sp\n",
    " </pre>\n",
    "</font>\n",
    "\n",
    "We can do the same for 'STAR's and 'GALAXY's as well. These data are all provided in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import copy\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1 Photometric Redshift Estimation\n",
    "\n",
    "* This is a regression problem, to be able to predict the redshift response variable given a number of photometric measurement \"features\"\n",
    "\n",
    "\n",
    "* The SDSS spectroscopic quasar sample provides a _training set_ for the prediction of photometric redshifts\n",
    "\n",
    "\n",
    "* Let's read in our SDSS quasars, and munge them into machine learning inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "qsos = pd.read_csv(\"../../data/qso10000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"spec_z\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\"])\n",
    "\n",
    "# Clean out extreme colors and bad magnitudes:\n",
    "qsos = qsos[(qsos[\"dered_r\"] > -9999) & (qsos[\"g_r_color\"] > -10) & (qsos[\"g_r_color\"] < 10)]\n",
    "\n",
    "\n",
    "# Response variables: redshift\n",
    "qso_redshifts = qsos[\"spec_z\"]\n",
    "\n",
    "# Features or attributes: photometric measurements\n",
    "qso_features = copy.copy(qsos)\n",
    "del qso_features[\"spec_z\"]\n",
    "qso_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visual Exploration\n",
    "\n",
    "* Over what redshift range do we have quasar spectra?\n",
    "\n",
    "* What structure is there in the _photometric feature space_ that might help us predict redshift?\n",
    "\n",
    "\n",
    "Let's plot all the features, colored by the target redshift, to look for structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bins =  hist(qso_redshifts.values,bins=100) ; xlabel(\"redshift\") ; ylabel(\"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Truncate the color at z=2.5 just to keep some contrast.\n",
    "norm = mpl.colors.Normalize(vmin=min(qso_redshifts.values), vmax=2.5)\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# Plot everything against everything else:\n",
    "rez = pd.scatter_matrix(qso_features[0:1000], alpha=0.2, figsize=[15,15], c=m.to_rgba(qso_redshifts.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Data and Target Values\n",
    "\n",
    "Now we have our machine learning inputs (photometric features: colors and magnitudes) and outputs (target redshift values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = qso_features.values  # Data: 9-d feature space\n",
    "y = qso_redshifts.values # Target: redshifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Design matrix shape =\", X.shape)\n",
    "print(\"Response variable vector shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression\n",
    "\n",
    "Let's follow the same procedure as in the [previous chunk (from the `SciKit-Learn` Linear Regression tutorial)](machinelearning2.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Instantiate a \"linear model\"\n",
    "from sklearn import linear_model\n",
    "linear = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the model, using all the attributes:\n",
    "linear.fit(X_train, y_train)\n",
    "\n",
    "# Do the prediction on the test data:\n",
    "y_lr_pred = linear.predict(X_test)\n",
    "\n",
    "# How well did we do? Compute MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_linear = np.sqrt(mean_squared_error(y_test,y_lr_pred))\n",
    "r2_linear = linear.score(X_test, y_test)\n",
    "print(\"Linear regression: MSE = \",mse_linear)\n",
    "print(\"R2 score =\",r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(y_test, y_lr_pred, 'o', alpha=0.2)\n",
    "title(\"Linear Regression - MSE = %.2f\" % mse_linear, fontsize=18)\n",
    "xlabel(\"Spectroscopic Redshift\", fontsize=18)\n",
    "ylabel(\"Photometric Redshift\", fontsize=18)\n",
    "plot([0,7],[0,7], color='red')\n",
    "ylim(0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just how bad is this? Here's the MSE from guessing the *average redshift of the training set* for all new objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive MSE\", ((1./len(y_train))*(y_train - y_train.mean())**2).sum())\n",
    "print(\"Linear regression: MSE = \",mse_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### *k*-Nearest Neighbor (KNN) Regression\n",
    "\n",
    "Now let's try a non-parametric model:\n",
    "\n",
    "> [\"Regression based on k-nearest neighbors.](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.\"\n",
    "\n",
    "\n",
    "#### Question:\n",
    "\n",
    "What underlying model is implied by the KNN algorithm? How many hidden parameters does it (effectively) have? What choices do we get to make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled = preprocessing.scale(X) # Many methods work better on re-scaled (\"whitened\") X.\n",
    "\n",
    "KNN = neighbors.KNeighborsRegressor(5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "KNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_knn_pred = KNN.predict(X_test)\n",
    "mse_knn = mean_squared_error(y_test,y_knn_pred)\n",
    "r2_knn = KNN.score(X_test, y_test)\n",
    "print(\"MSE (KNN) =\", mse_knn)\n",
    "print(\"R2 score (KNN) =\",r2_knn)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (linear regression) = \",mse_linear)\n",
    "print(\"R2 score (linear regression) =\",r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(y_test, y_knn_pred,'o',alpha=0.2)\n",
    "title(\"k-NN Residuals - MSE = %.2f\" % mse_knn, fontsize=18)\n",
    "xlabel(\"Spectroscopic Redshift\", fontsize=18)\n",
    "ylabel(\"Photometric Redshift\", fontsize=18)\n",
    "plot([0,7], [0,7], color='red')\n",
    "ylim(0,5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning the KNN Model\n",
    "\n",
    "* Let's vary the control parameters of the KNN model, to see how good we can make our predictions.\n",
    "\n",
    "* We can see our options in the model `repr`:\n",
    "\n",
    "```\n",
    "KNeighborsRegressor(algorithm='auto',\n",
    "    leaf_size=30, metric='minkowski',      \n",
    "    metric_params=None, n_neighbors=5, \n",
    "    p=2, weights='uniform')\n",
    "```\n",
    "* Let's first make a \"validation curve\" to investigate one parameter: the number of nearest neighbors averaged over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We'll vary the number of neighbors used:\n",
    "param_name = \"n_neighbors\"\n",
    "param_range = np.array([1,2,4,8,16,32,64])\n",
    "\n",
    "# Compute our cv scores for the above range of the no. of neighbors:\n",
    "from sklearn.model_selection import validation_curve\n",
    "training_scores, validation_scores = validation_curve(KNN, X_scaled, y,\n",
    "                                                      param_name=param_name,\n",
    "                                                      param_range=param_range, \n",
    "                                                      cv=3, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_validation_curve(param_name,parameter_values, training_scores, validation_scores):\n",
    "    training_scores_mean = np.mean(training_scores, axis=1)\n",
    "    training_scores_std = np.std(training_scores, axis=1)\n",
    "    validation_scores_mean = np.mean(validation_scores, axis=1)\n",
    "    validation_scores_std = np.std(validation_scores, axis=1)\n",
    "\n",
    "    plt.fill_between(parameter_values, training_scores_mean - training_scores_std,\n",
    "                     training_scores_mean + training_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(parameter_values, validation_scores_mean - validation_scores_std,\n",
    "                     validation_scores_mean + validation_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(parameter_values, training_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training R2 score\")\n",
    "    plt.plot(parameter_values, validation_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation R2 score\")\n",
    "    plt.ylim(validation_scores_mean.min() - .1, training_scores_mean.max() + .1)\n",
    "    plt.xlabel(param_name, fontsize=18)\n",
    "    plt.legend(loc=\"best\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_validation_curve(param_name, param_range, training_scores, validation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Question:\n",
    "\n",
    "Can you explain the shapes of these two curves? Talk to your neighbor for a few minutes, and be prepared to suggest reasons for a) the rise and fall of the cross validation score and b) the monotonic decrease in training score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model tuning with `GridSearchCV`\n",
    "\n",
    "* Now, let's see if we can do better by varying some other KNN options as well - in a *grid search*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': np.array([1,2,4,8,16,32,64]),\n",
    "                  'weights': ['uniform','distance'],\n",
    "                       'p' : np.array([1,2])}\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "KNN_tuned = GridSearchCV(KNN, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A `GridSearchCV` object behaves just like a model, except it carries out a cross-validation while fitting:\n",
    "\n",
    "<img src=\"../../lessons/graphics/ml_grid_search_cross_validation.svg\" width=70% align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fit will take a minute or so, as it loops over all the search positions in the hyper-parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "KNN_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_knn_tuned_pred = KNN_tuned.predict(X_test)\n",
    "\n",
    "mse_knn_tuned = mean_squared_error(y_test,y_knn_tuned_pred)\n",
    "r2_knn_tuned = KNN_tuned.score(X_test, y_test)\n",
    "\n",
    "print(\"MSE (tuned KNN) =\", mse_knn_tuned)\n",
    "print(\"R2 score (tuned KNN) =\", r2_knn_tuned)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (KNN) = \", mse_knn)\n",
    "print(\"R2 score (KNN) =\", r2_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which are the best KNN control parameters we found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_tuned.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This value of `n_neighbors` is consistent with the peak in cross-validation score in the validation curve plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalization Error\n",
    "\n",
    "Notice that all the above tuning happened while training on a single split (`X_train` and `y_train`).\n",
    "\n",
    "\n",
    "It's possible that that one particular fold prefers a slightly different set of model parameters than a different one - so to assess our generalization error, we need a further level of cross-validation.\n",
    "\n",
    "\n",
    "We can do this by passing a `GridSearchCV` model to the cross validation score calculator. This will take a few minutes, as the grid search is carried out for each CV fold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "R2 = cross_val_score(KNN_tuned, X_scaled, y, cv=3, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "meanR2, errR2 = np.mean(R2), np.std(R2)/np.sqrt(len(R2))\n",
    "print('Mean score:', meanR2, '+/-', errR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "* Optimizing over control parameters (or hyper parameters) with grid search cross validation is a form of model selection.\n",
    "\n",
    "\n",
    "* When presented with new data samples (photometry), and asked to predict the target response variables (photometric redshifts), we'll need a trained machine that has not been *over-fitted* to the training data.\n",
    "\n",
    "\n",
    "* Minimizing and estimating the generalization error is a way to reduce the risk of getting this prediction wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's finish off our photo-z machine learning tool, by:\n",
    "\n",
    "\n",
    "* Choosing the best combination of hyper-parameters (from our cross-validation analysis)\n",
    "\n",
    "* Training it on the whole of the training dataset\n",
    "\n",
    "* Using it to predict the redshifts of the objects in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNz = KNN_tuned.best_estimator_\n",
    "\n",
    "KNNz.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 571\n",
    "one_new_quasar = X_test[j,:].reshape(1, -1)\n",
    "zphoto = KNNz.predict(one_new_quasar)\n",
    "zspec = y_test[j]\n",
    "print(\"True redshift cf. KNN photo-z:\", zspec, ' cf.', zphoto[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "zspec = y_test\n",
    "zphoto = KNNz.predict(X_test)\n",
    "\n",
    "plot(zspec, zphoto,'o',alpha=0.1)\n",
    "title(\"KNNz performance\", fontsize=18)\n",
    "xlabel(\"Spectroscopic Redshift\", fontsize=18)\n",
    "ylabel(\"Photometric redshift\", fontsize=18)\n",
    "plot([0,7], [0,7], color='red')\n",
    "ylim(0,5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2 Quasar Classification with Random Forest\n",
    "\n",
    "* Let's switch gears and do a 3-class classification problem: star, galaxy, or QSO.\n",
    "\n",
    "\n",
    "* A very good general-purpose classification (and regression!) algorithm is \"Random Forest.\" See [this yhat blog post](http://blog.yhathq.com/posts/random-forests-in-python.html) for a nice high level introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forests\n",
    "\n",
    "\n",
    "* From the `Scikit-Learn` docs: [\"A random forest is a meta estimator that fits a number of *decision tree classifiers* on various sub-samples of the dataset, and uses averaging to improve the predictive accuracy and control over-fitting.\"](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "\n",
    "* Decision trees encode sequences of \"cuts\" in the data, leading to the separation of the samples into groups to which labels are assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forests of Random Decision Trees\n",
    "\n",
    "\n",
    "* Random decision trees are random in that they:\n",
    "\n",
    "  * Are set up with random cut thresholds, and random feature-ordering\n",
    "  \n",
    "  * Are given random bootstrap sub-samples of the data\n",
    "  \n",
    "  \n",
    "* Most trees in a random forest are _terrible_ classifiers: but we only need a few with high weight to dominate the average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quasar Classification with Random Forest\n",
    "\n",
    "* Let's read in equal numbers (1000) of all three types of data, clean them up, and set $y$ equal to the classification label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_sources = pd.read_csv(\"../../data/qso10000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"])[:1000]\n",
    "\n",
    "all_sources = all_sources.append(pd.read_csv(\"../../data/star1000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"]))\n",
    "\n",
    "all_sources = all_sources.append(pd.read_csv(\"../../data/galaxy1000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"]))\n",
    "\n",
    "all_sources = all_sources[(all_sources[\"dered_r\"] > -9999) & (all_sources[\"g_r_color\"] > -10) & (all_sources[\"g_r_color\"] < 10)]\n",
    "\n",
    "all_labels = all_sources[\"class\"]\n",
    "\n",
    "all_features = copy.copy(all_sources)\n",
    "del all_features[\"class\"]\n",
    "\n",
    "X = copy.copy(all_features.values)\n",
    "y = copy.copy(all_labels.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Data and Target Labels\n",
    "\n",
    "In classification problems the target values are _categorical_ \"labels\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Feature vector shape =\", X.shape)\n",
    "print(\"Class label vector shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(y, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visual Exploration\n",
    "\n",
    "What structure can we see in the data? Let's plot all the features as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = y.copy()\n",
    "yy[yy==\"QSO\"] = 0.0    # Red\n",
    "yy[yy==\"STAR\"] = 0.5   # Green\n",
    "yy[yy==\"GALAXY\"] = 1.0 # Blue\n",
    "yy = yy.astype(float)\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=min(yy), vmax=max(yy))\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "rez = pd.plotting.scatter_matrix(all_features, alpha=0.2, figsize=[15,15], c=m.to_rgba(yy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest feature importances\n",
    "\n",
    "* Each decision tree in the Random Forest focuses on a different combination of features\n",
    "\n",
    "* One output of the fitted model is an indication of which features are most important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(all_sources.columns.values,rf.feature_importances_),key=lambda q: q[1],reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Accuracy\n",
    "\n",
    "* The accuracy of a classifier is the _fraction of predictions made that are correct_. \n",
    "\n",
    "* Ensemble classifiers like Random Forest provide an `oob_score`, the mean \"Out of Bag\" prediction accuracy\n",
    "\n",
    "* Each decision tree in the ensemble is only working on a subset of the data, so it can track its accuracy with the data not in its own bag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Report the \"out of bag\" accuracy score:\n",
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This score _looks_ good, but this is the accuracy on the _training set_.\n",
    "\n",
    "\n",
    "#### Question:\n",
    "How should we estimate the generalized accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classifier Tuning with GridSearchCV\n",
    "\n",
    "As before, this will take a few minutes, as the model selection is carried out on each fold within the training set... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter values to try:\n",
    "parameters = {'n_estimators':(50,100,200), \"max_features\": [\"auto\",3],\n",
    "              'criterion':[\"gini\",\"entropy\"], \"min_samples_leaf\": [1,2]}\n",
    "\n",
    "# Training/test split:\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Do a grid search to find the highest 3-fold cross-validation score:\n",
    "rf_tuned = GridSearchCV(rf, parameters, cv=3, verbose=1)\n",
    "RFselector = rf_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Print the best score and estimator:\n",
    "print('Best OOB score:', RFselector.best_score_)\n",
    "print(RFselector.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question:\n",
    "\n",
    "Would you be satisfied with a 95% successful classification fraction? Can you suggest some more useful scores to optimize? (Hint: imagine using a classifier to select a sample of *follow-up targets*.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confusion Matrices\n",
    "\n",
    "One way of visualizing classification accuracy across a range of labels is via a *confusion matrix*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions given the test data:\n",
    "y_pred = RFselector.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix', fontsize=18)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label', fontsize=18)\n",
    "plt.xlabel('Predicted label', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC Curves\n",
    "\n",
    "Each Random Forest output label comes with a *classification probability*, computed from the results of the whole forest. \n",
    "\n",
    "To select a sample of classified objects, one can choose a selection threshold in this class probability, and only keep objects with higher probability than this threshold.\n",
    "\n",
    "\n",
    "The availability of a class probability leads to an important diagnostic: the \"Receiver Operating Characteristic\" or [\"ROC\" curve](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "The ROC curve shows a classifier's *true positive rate* (TPR) plotted against the *false positive rate* (FPR), as the selection threshold is varied.\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;{\\rm TPR} = \\frac{TP}{TP + FN}$  the fraction of actual 1's that are classified as 1's.\n",
    "\n",
    "\n",
    "$\\;\\;\\;\\;\\;{\\rm FPR} = \\frac{FP}{FP + TN}$  the fraction of actual 0's that are classified as 1's.  \n",
    "\n",
    "> TPR = \"Recall\", \"Completeness\". FPR = \"Fall-out\".\n",
    "> In astronomy, we are also (very) interested in \"Precision,\" $\\frac{TP}{FP + TP}$, which is related to \"Purity\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Typically, classifiers have control parameters that affect both the TPR and FPR (often improving one at the expense of the other), so the ROC curve is a good tool for investigating these parameters. \n",
    "\n",
    "\n",
    "Likewise, ROC curves provide a very good way to compare different classifiers, via the \"area under the curve\" (and above the random classifier TPR=FPR line). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's use the `SciKit-Learn` utilities to plot an ROC curve for our RFselector, and quantify its performance via the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the test data, and store the classification probabilities:\n",
    "BestRFselector = RFselector.best_estimator_\n",
    "y_prob = BestRFselector.fit(X_train, y_train).predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and area under curve (AUC) for each class:\n",
    "labels = BestRFselector.classes_ # ['GALAXY', 'QSO', 'STAR'] - the order is decided by the machine!\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i,label in enumerate(labels):\n",
    "    fpr[label], tpr[label], _ = roc_curve(y_test, y_prob[:, i], pos_label=label)\n",
    "    roc_auc[label] = auc(fpr[label], tpr[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lw = 2\n",
    "colors = {'QSO':'red', 'STAR':'green', 'GALAXY':'blue'}\n",
    "for label in labels:\n",
    "    plot(fpr[label], tpr[label], color=colors[label],\n",
    "         lw=lw, label='%s (AUC = %0.3f)' % (label, roc_auc[label]))\n",
    "plot([0, 1], [0, 1], color='gray', lw=lw, linestyle='--')\n",
    "xlim([0.0, 1.0])\n",
    "ylim([0.0, 1.05])\n",
    "xlabel('False Positive Rate', fontsize=18)\n",
    "ylabel('True Positive Rate', fontsize=18)\n",
    "title('RFselector ROC Curve', fontsize=18)\n",
    "legend(loc=\"lower right\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Endnotes\n",
    "\n",
    "\n",
    "The `scikit-learn` package makes it easy to experiment with various machine learning algorithms, and make non-parametric models that (via cross-validation) have high generalized prediction accuracy.\n",
    "\n",
    "Other frameworks exist: Google's [\"TensorFlow\"](https://www.tensorflow.org/tutorials/deep_cnn) is worth investigating, both for general ML, and also for its _neural network_ support. PGMs, as well as the ML basics shown here, will reappear. \n",
    "\n",
    "These models may not provide parameter uncertainties or even much new understanding of the dataset\n",
    "- This may not be your priority in a given application.\n",
    "- If it is, you'll have more work to do to enable you to draw scientific conclusions...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
