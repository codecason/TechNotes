{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical Frameworks\n",
    "\n",
    "Goals:\n",
    "\n",
    "* Explore the relationship between \"characterizing the posterior PDF\" and \"fitting a model to data.\"\n",
    "\n",
    "* Understand how to derive maximum likelihood estimators and their confidence intervals\n",
    "\n",
    "* Be able to compare, contrast and appreciate the Bayesian and Frequentist approaches to statistics\n",
    "\n",
    "We'll work through a simple Bayesian inference (fitting a straight line and reporting summaries of marginalized posterior PDFs), and then compare with the frequentist procedure of _estimating_ the gradient and intercept and reporting confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "* Ivezic 3.2.2, 4.1 and 4.2\n",
    "\n",
    "* [Vanderplas (2014), \"Frequentism and Bayesianism: A Python-driven Primer\"](https://arxiv.org/abs/1411.5018)\n",
    "\n",
    "* [Hogg, Bovy & Lang (2010), \"Data analysis recipes: Fitting a model to data\"](https://arxiv.org/abs/1008.4686)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The period-magnitude relation in Cepheid stars\n",
    "\n",
    "* Cepheids are stars whose brightness oscillates with a stable period the logarithm of which appears to be strongly correlated with their mean luminosity (or absolute magnitude).\n",
    "\n",
    "<img width=\"512\" src=\"graphics/Cepheid-variabledb92-600x461.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* A lot of _monitoring_ data - repeated imaging and subsequent \"photometry\" of the star - can provide a measurement of the absolute magnitude (if we know the distance to it's host galaxy) and the period of the oscillation.\n",
    "\n",
    "* Let's look at some Cepheid measurements reported by [Riess et al (2011, R11)](https://arxiv.org/abs/1103.2976).  The data are in the form of datapoints, one for each star.\n",
    "\n",
    "* The periods are well measured, while the magnitudes come with reported error bars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"graphics/cepheid_data.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The model, and the data\n",
    "\n",
    "* Let's assume that Cepheid stars' luminosities are related to their oscillation periods by a power law, such that their apparent magnitude and log period follow the linear relation\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;m = a\\;\\log_{10} P + b$\n",
    "\n",
    "* The data consist of *observed magnitudes with quoted uncertainties*, such as: \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;m^{\\rm obs} = 24.51 \\pm 0.31$ at $\\log_{10} P = \\log_{10} (13.0/{\\rm days})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian inference\n",
    "\n",
    "* We compute the posterior PDF for the parameters $a$ and $b$ given the data and the assumed model $H$: \n",
    "\n",
    "$\\;\\;\\;\\;\\;P(a,b|\\boldsymbol{m}^{\\rm obs},H) \\propto P(\\boldsymbol{m}^{\\rm obs}|a,b,H)\\;P(a,b|H)$\n",
    "\n",
    "* We can evaluate the unnormalized posterior PDF on a grid, renormalize it numerically, and then visualize and summarize the resulting 2D function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probabilistic graphical model\n",
    "\n",
    "* Let's draw a PGM for this inverse problem, imagining our way through what we would do to generate a mock dataset like the one we have from R11.\n",
    "\n",
    "* If we were generating mock data, then for any plausible choice of parameters $a$ and $b$ we can predict the true magnitude $m_k$ of each star given its period $P_k$, and then add noise to simulate each observed magnitude $m^{\\rm obs}_k$.\n",
    "\n",
    "#### Exercise:  Draw the PGM for this problem with your neighbor, and be prepared to point out its features in 5 minutes' time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PGM\n",
    "\n",
    "<img width=\"400\" src=\"graphics/pgms_cepheids.png\"/>\n",
    "\n",
    "> NB. The magnitude uncertainties $\\sigma^{\\rm obs}$ are given to us in the data file; we can use them as-is if we believe them. The \"true\" magnitudes $m$ are _determined_ by our power law model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building an inference\n",
    "\n",
    "Now let's assign PDFs for each node in the PGM, and derive the unnormalized posterior PDF for $a$ and $b$.\n",
    "\n",
    "We'll need:\n",
    "\n",
    "* The sampling distribution: $P(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},H)$\n",
    "\n",
    "* The conditional PDF for the latent variables $m_k$, $P(m_k|a,b,\\log_{10}{P_k},H)$\n",
    "\n",
    "* A prior PDF for our parameters: $P(a,b|H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The sampling distribution $P(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},H)$\n",
    "\n",
    "We were given points ($m^{\\rm obs}_k$) with error bars ($\\sigma_k$), which suggests a *Gaussian* sampling distribution for each one:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;P(m^{\\rm obs}_k|m_k,\\sigma_k,H) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} \\exp{-\\frac{(m^{\\rm obs}_k - m_k)^2}{2\\sigma_k^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Note that we are never _given_ the form of the sampling distribution: it always has to be assumed. It turns out that the Gaussian distribution is the \"Maximum Entropy\" (minimally informative) choice given the information provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we assume that the measurements of each Cepheid start are *independent* of each other, then we can define *predicted and observed data \"vectors\"* $\\boldsymbol{m}$ and $\\boldsymbol{m}^{\\rm obs}$ (plus a corresponding observational uncertainty \"vector\" $\\boldsymbol{\\sigma}$) and compute the joint sampling distribution as:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;P(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},\\boldsymbol{\\sigma},H) = \\prod_k P(m^{\\rm obs}_k|m_k,\\sigma_k,H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The conditional PDF $P(m_k|a,b,\\log_{10}{P_k},H)$ \n",
    "\n",
    "Our relationship between the intrinsic magnitude and the log period is linear and _deterministic_, indicating the following *delta-function* PDF:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;P(m_k|a,b,\\log_{10}{P_k},H) = \\delta(m_k - a\\log_{10}{P_k} - b)$\n",
    "\n",
    "<img width=\"300\" src=\"graphics/pgms_cepheids.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The resulting joint likelihood, $\\mathcal{L}(a,b;\\boldsymbol{m}^{\\rm obs}) = P(\\boldsymbol{m}^{\\rm obs}|a,b,H)$\n",
    "\n",
    "The PDF for everything inside the PGM plate is the following product:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;P(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},\\sigma,H)\\;P(\\boldsymbol{m}|a,b,H)$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = \\prod_k P(m^{\\rm obs}_k|m_k,\\sigma_k,H)\\;\\delta(m_k - a\\log_{10}{P_k} - b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Marginalizing out the latent variables\n",
    "\n",
    "The intrinsic magnitudes of each Cepheid $m_k$ are \"latent variables,\" to be _marginalized out_:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;P(\\boldsymbol{m}^{\\rm obs}|a,b,H) = \\int P(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},\\sigma,H)\\;P(\\boldsymbol{m}|a,b,H)\\; d\\boldsymbol{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = \\prod_k \\int P(m^{\\rm obs}_k|m_k,\\sigma_k,H)\\;\\delta(m_k - a\\log_{10}{P_k} - b) dm_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\;\\;\\;\\;\\;\\;\\; \\longrightarrow P(\\boldsymbol{m}^{\\rm obs}|a,b,H) = \\prod_k P(\\boldsymbol{m}^{\\rm obs}_k|(a\\log{P_k} + b),\\sigma_k,H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The log likelihood\n",
    "\n",
    "Taking logs, for numerical stability, the product in the joint likelihood becomes the following sum:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\log P(\\boldsymbol{m}^{\\rm obs}|a,b,H) = \\sum_k \\log P(m^{\\rm obs}_k|(a\\log{P_k} + b),\\sigma,H)$\n",
    "\n",
    "which, substituting in our Gaussian form, gives us: \n",
    "\n",
    "$\\;\\log P(\\boldsymbol{m}^{\\rm obs}|a,b,H) = -\\frac{1}{2}\\sum_k \\log{2\\pi\\sigma_k^2} - \\frac{1}{2} \\sum_k \\frac{(m^{\\rm obs}_k - a\\log{P_k} - b)^2}{\\sigma_k^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Note that the log likelihood $\\log P(\\boldsymbol{m}^{\\rm obs}|a,b,H)$ is a function, $\\log \\mathcal{L}(a,b;\\boldsymbol{m}^{\\rm obs})$ that can be evaluated, as a function of $a$ and $b$, at constant $\\boldsymbol{m}^{\\rm obs}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Chi-squared misfit statistic\n",
    "\n",
    "Astronomers often call the term in the log likelihood that depends on the parameters $\\chi^2$ (\"chi-squared\"):\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\chi^2 = \\sum_k \\frac{(m^{\\rm obs}_k - a\\log{P_k} - b)^2}{\\sigma_k^2}$\n",
    "\n",
    "$\\chi^2$ is a \"misfit\" statistic, that quantifies the difference between \"observed and predicted data.\" Under our assumptions, it's equal to -2 times the log likelihood (up to a constant). The \"predicted data\" are $m_k = a\\log{P_k} - b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Including the prior $P(a,b|H)$\n",
    "\n",
    "* Let's assume  the prior PDFs for $a$ and $b$ to be independent, such that $P(a,b|H) = P(a|H)P(b|H)$.\n",
    "\n",
    "* For now, let's assume uniform prior PDFs for both $a$ and $b$, supposing that we know roughly what size they are:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;P(a|H) = \\frac{1}{a_{\\rm max} - a_{\\rm min}}$ with $(a_{\\rm min}, a_{\\rm max}) = (-10, 10)$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;P(b|H) = \\frac{1}{b_{\\rm max} - b_{\\rm min}}$ with $(b_{\\rm min}, b_{\\rm max}) = (10, 30)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Putting it all together\n",
    "\n",
    "The joint PDF is:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;P(\\boldsymbol{m}^{\\rm obs},a,b|H) = P(\\boldsymbol{m}^{\\rm obs}|a,b,H) P(a|H) P(b|H)$\n",
    "\n",
    "> Since we marginalized out the $m$, analytically, we _could_ have drawn the PGM more simply, jumping directly to $P(\\boldsymbol{m}^{\\rm obs}|a,b,H)$. However, it's often helpful to _explicitly_ distinguish between \"true\" parameters and latent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Characterizing the posterior PDF\n",
    "\n",
    "With the completed factorization of the joint PDF for all variables, we have the following product: \n",
    "\n",
    "$\\;\\;P(a,b|\\boldsymbol{m}^{\\rm obs},H) \\propto P(\\boldsymbol{m}^{\\rm obs}|a,b,H) P(a|H) P(b|H)$\n",
    "\n",
    "We can evaluate the posterior PDF $P(a,b|\\boldsymbol{m}^{\\rm obs},H)$ for any choice of parameters $(a,b)$, up to a normalization constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Posterior Evaluation on a Grid\n",
    "\n",
    "* Our 2-D posterior PDF can be visualized as a contour plot\n",
    "\n",
    "* We can choose contours that display the _credible regions_ that enclose 68% and 95% of the posterior probability.\n",
    "\n",
    "* _Given our assumption that the model is true, the probability that the true values of the model parameters lie within the 95% credible region given the data is 0.95._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"graphics/cepheids_2d-posterior.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing our inferences\n",
    "\n",
    "* Typically, we will want to (or will be expected to) report \"answers\" for our model parameters\n",
    "\n",
    "* This can be difficult: our result _is_ the posterior PDF for the model parameters given the data!\n",
    "\n",
    "* A convenient, and in this case appropriate, choice is to report quantiles of the 1D marginalized PDFs\n",
    "\n",
    "> In general, the most important thing when summarizing inferences is to state clearly what you are doing, preferably with critical commentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1D marginalized posterior PDFs\n",
    "\n",
    "<img src=\"graphics/cepheids_1d-posteriors.png\">\n",
    "\n",
    "Let's define the 68% credible interval as the range between the 16th and 84th percentile, and quote it as an (asymmetric) \"error bar\" on the 1-D posterior medians:\n",
    "\n",
    "$a = -2.95^{+0.06}_{-0.06} \\;\\;\\;\\;\\;\\;\\; b =  26.27^{+0.09}_{-0.1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notes on summaries of marginal PDFs \n",
    "\n",
    "* In this simple case, our report makes sense: the medians of both 1D marginalized PDFs lie within the region of high 2D posterior PDF. *This will not always be the case.*\n",
    "\n",
    "\n",
    "* The marginalized 1-D posterior for $x$ has a well-defined meaning, regardless of the higher dimensional structure of the joint posterior:  it is $P(x|d,H)$, the PDF for $x$ given the data and the model, and *accounting for the uncertainty in all other parameters*.\n",
    "\n",
    "\n",
    "* The posterior PDF we computed is close to, but not quite, a bivariate Gaussian. \n",
    "\n",
    "**Question: What choice of (proper) prior would we have had to make in order for the posterior PDF to be _exactly_ Gaussian?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Post-inference model checking\n",
    "\n",
    "\n",
    "It's always a good idea to check that the inferred parameter values are sensible: look at the model predictions in data space.\n",
    "\n",
    "$\\longrightarrow$ Let's overlay the model period-magnitude relation defined by the 1-D posterior median parameter values on the data.\n",
    "\n",
    "<img src=\"graphics/cepheids_posterior-median-check.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Fitting the data\"\n",
    "\n",
    "* The Bayesian solution is not a single set of \"best-fit\" parameters. \n",
    "\n",
    "* We can think of the posterior PDF as providing us with a continuous distribution of model fits that are _plausible_ given the data and our assumptions.\n",
    "\n",
    "* There are other ways of defining the parameters that _best fit_ the data: the primary one is \"the method of Maximum Likelihood\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum likelihood\n",
    "\n",
    "* Instead of asking for the posterior probability for the parameters given the data, $P(a,b|\\boldsymbol{m}^{\\rm obs},H)$, we could find the parameters that maximize the probability of getting the data: $P(\\boldsymbol{m}^{\\rm obs}|a,b,H)$\n",
    "\n",
    "  > In astronomy, \"best fit\" often (but not always) means \"maximum likelihood\"\n",
    "\n",
    "* Where does the emphasis on the likelihood, rather than the posterior come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequentism\n",
    "\n",
    "* In the frequentist school of statistics, parameters do not have probability distributions. Probability can only be used to describe _frequencies_, not _degrees of belief_ (or odds). \n",
    "\n",
    "* In the frequentist view, it's only the data that can be modeled as having been drawn from a probability distribution, because we can imagine doing the experiment or observation multiple times, and building up a _frequency_ distribution of results.\n",
    "\n",
    "* This PDF over datasets is the sampling distribution, e.g. $P(\\boldsymbol{m}^{\\rm obs}|a,b,H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Given an assumed model, the frequentist view is that there is only one set of parameters, the true ones, and our job is to _estimate_ them.\n",
    "\n",
    "* Derivation of good estimators is a major activity in frequentist statistics, and has led to some powerful mathematical results and fast computational shortcuts - _some of which are useful in Bayesian inference too_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Likelihood Principle\n",
    "\n",
    "* The _likelihood principle_ holds that all of the information in the data that is relevant to the model parameters is contained in the likelihood function $\\mathcal{L}(a,b;\\boldsymbol{m}^{\\rm obs}) = P(\\boldsymbol{m}^{\\rm obs}|a,b,H)$\n",
    "\n",
    "> This was evident in our Bayesian treatment, PGMs etc too: Frequentists and Bayesians are in full agreement about the importance of the likelihood function!\n",
    "\n",
    "* As a result of this focus, Maximum Likelihood estimators (MLEs) have some good properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum likelihood estimators\n",
    "\n",
    "* **Consistency:** as more data are taken, the MLE tends towards the true parameter value if the model is correct. \n",
    "\n",
    "> MLEs can be \"biased\" but this bias goes to zero as $N_{\\rm data} \\rightarrow \\infty$ \n",
    "\n",
    "* **Efficiency:** among estimators, MLEs have the minimum variance when sampled over datasets\n",
    "\n",
    "* **Asymptotic Normality:** as the dataset size increases, the distribution of MLEs over datasets tends to a Gaussian centred at the true parameter value.\n",
    "\n",
    "> The covariance of this ultimate Gaussian distribution is the inverse of the \"Fisher information matrix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In our Cepheid straight line fit, we can derive MLEs for our parameters by finding the maximum (log) likelihood parameters analytically\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\log L(a,b) = \\log P(\\boldsymbol{m}^{\\rm obs}|a,b,H) = -\\frac{1}{2}\\sum_k \\log{2\\pi\\sigma_k^2} - \\frac{1}{2} \\sum_k \\frac{(m^{\\rm obs}_k - a\\log{P_k} - b)^2}{\\sigma_k^2}$\n",
    "\n",
    "* That is, we need to find the parameters $(\\hat{a}, \\hat{b})$ that give\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\; -2 \\nabla \\log L(a,b) = \\nabla\\,\\chi^2$ = 0\n",
    "\n",
    "> NB: Maximizing a Gaussian likelihood is equivalent to minimizing $\\chi^2$ - and gives a \"weighted least squares\" fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The result in this case is a pair of equations that we can solve for the best-fit parameters $(\\hat{a}, \\hat{b})$, that give the smallest misfit between observed and model-predicted data\n",
    "\n",
    "Writing $x = \\log{P}$ and $y = m^{\\rm obs}$, we have\n",
    "\n",
    "$\\frac{\\partial \\log L}{\\partial a}\\Bigr|_{\\hat{a},\\hat{b}} =  \\sum_k \\frac{x_k(y_k - \\hat{a}x_k - \\hat{b})}{\\sigma_k^2} = 0 \\longrightarrow   \\hat{a} \\sum_k \\frac{x_k^2}{\\sigma_k^2} + \\hat{b} \\sum_k \\frac{x_k}{\\sigma_k^2} = \\sum_k \\frac{x_k y_k}{\\sigma_k^2}$\n",
    "\n",
    "$\\frac{\\partial \\log L}{\\partial b}\\Bigr|_{\\hat{a},\\hat{b}} =  \\sum_k \\frac{(y_k - \\hat{a}x_k - \\hat{b})}{\\sigma_k^2} = 0 \\longrightarrow \\hat{a} \\sum_k \\frac{x_k}{\\sigma_k^2} + \\hat{b} \\sum_k \\frac{1}{\\sigma_k^2} = \\sum_k \\frac{y_k}{\\sigma_k^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This set of linear equations can be solved straightforwardly to find the _estimators_ $\\hat{a}$ and $\\hat{b}$:\n",
    "\n",
    "$\\begin{pmatrix} S_{xx} & S_{x} \\\\ S_x & S_0 \\end{pmatrix} \\begin{pmatrix} \\hat{a} & \\hat{b} \\end{pmatrix} = \\begin{pmatrix} S_{xy} \\\\ S_{y} \\end{pmatrix}$\n",
    "\n",
    "> All the information in the data that is needed to find the best-fit parameters $\\boldsymbol{\\hat{\\theta}}$ is contained in a set of so-called **sufficient statistics** $S_{xx}$, $S_y$ etc. This is a common feature of maximum likelihod estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computing and combining the sufficient statistics, the maximum likelihood estmimators are as follows:\n",
    "\n",
    "$ \\hat{a} = -2.95 $\n",
    "\n",
    "$ \\hat{b} = 26.26 $\n",
    "\n",
    "**Questions: Why do you think the MLE $\\hat{b}$ not exactly the same as the posterior median value for $b$?**\n",
    "\n",
    "**Under what circumstances might $\\hat{b}$ or $\\hat{a}$ be _very_ different from the posterior medians?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributions of estimators\n",
    "\n",
    "* In frequentism, we think of the estimators having distributions, since each dataset that we imagine being drawn from the sampling distribution will produce one estimator. An ensemble of (hypothetical) datasets leads to a (hypothetical) distribution of estimators\n",
    "\n",
    "* One straightforward approximate way to estimate these distributions is to use the asymptotic normality property of MLEs, and associate a _Gaussian approximation to the likelihood_ with the Gaussian distribution for the MLEs we expect to see when averaging over datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The distribution of the log likelihood\n",
    "\n",
    "* An estimator can be thought of as a _summary_ of the data, and so can the value of the value of the log likelihood evaluated at the estimators. \n",
    "\n",
    "* The distribution of the log likelihood itself over the hypothetical ensemble of datasets provides a route to a _confidence interval._\n",
    "\n",
    "* In our simple Gaussian likelihood example, and also in the large dataset limit, twice the negative log likelihood (our $\\chi^2$ statistic) follows a $\\chi^2$ distribution with the same number of degrees of freedom as the dimensionality of the parameter space. Integrating this distribution from 0 to some boundary $\\Delta \\chi^2_{D}$ defines a confidence region in parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confidence regions from $\\chi^2$ distributions\n",
    "\n",
    "* For example: in 1D, the 68.3% confidence region is bounded by the contour at $\\chi^2_{\\rm min} + \\Delta \\chi^2_{D}$ where $\\Delta \\chi^2_{D} = 1$ in 1D, and $\\Delta \\chi^2_{D} = 2.30$ in 2D. \n",
    "\n",
    "* In the 1D case, the boundary of the 68.3% confidence interval lies 1 standard deviation (or \"1-sigma\") from the mean, while the 95.4% CI lies 2-sigma from the mean.\n",
    "\n",
    "> In general, $\\Delta \\chi^2_{D}$ can be computed from the $\\chi^2$ distribution quantile (or \"percentage point\") function, e.g.  `scipy.stats.chi2.ppf(0.683, D)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats, numpy as np\n",
    "\n",
    "dimensions = 2\n",
    "level = 0.683\n",
    "dchisq = scipy.stats.chi2.ppf(level, dimensions)\n",
    "\n",
    "np.round(dchisq,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In our example, we have that:\n",
    "\n",
    "### $\\;\\;\\;\\;\\;\\chi^2 = (\\boldsymbol{\\hat{\\theta}} - \\boldsymbol{\\theta})^T (\\mathcal{M}^T C^{-1} \\mathcal{M})^{-1} (\\boldsymbol{\\hat{\\theta}} - \\boldsymbol{\\theta})$, \n",
    "\n",
    "$\\;\\;\\;\\;\\;$where $C$ is the covariance matrix of the data (i.e. $C$ is diagonal, with elements equal to the squared uncertainties on each datapoint) and $\\mathcal{M}$ is the 2xN _design matrix_ that predicts data given parameters via   $\\mathcal{M}\\boldsymbol{\\theta} = \\boldsymbol{m}$.\n",
    "\n",
    "* This $\\chi^2$ function can be computed on a grid, and visualized as a contour plot: the contour at $\\chi^2_{\\rm min} + 2.30$ will enclose the 68% 2-D confidence region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximate uncertainties\n",
    "\n",
    "* In general, the covariance matrix of a Gaussian approximation to the likelihood can be calculated by _taking second derivatives of the log likelihood at the peak_, and inverting the resulting _Hessian_ matrix. \n",
    "\n",
    "* This gives a lower limit to the covariance of a set of estimators: \n",
    "\n",
    "## $\\;\\;\\;\\;\\;V^{-1}_{ij} \\geq -\\frac{\\partial^2 \\log{L}}{\\partial\\theta_i\\partial\\theta_j} \\biggr|_{\\boldsymbol{\\hat{\\theta}}}$\n",
    "\n",
    "> $V$ is what [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html) returns in its `hess_inv` field if you pass it the negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coping with multiple dimensions\n",
    "\n",
    "In Frequentism there is no concept of marginalization: parameters are assumed to be single-valued and fixed, and the only probability distribution considered is _P(data|pars)_, not _P(pars|data)_.\n",
    "\n",
    "Instead, 1D confidence intervals are usually derived by _maximizing the likelihood_ over the other parameters, in a _profile likelihood_ analysis. The repeated optimizations involved can be computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wording\n",
    "\n",
    "* In general, Frequentist confidence intervals are different from Bayesian credible regions:\n",
    "\n",
    "> \"68% of datasets would give a 68% frequentist confidence interval that contains the true parameter value\"\n",
    "\n",
    "> \"The probability of the true parameter value lying within the 68% Bayesian credible region is 68%\"\n",
    "\n",
    "* The difference in wording comes from the different ways that probability is used in the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uncertainties in the estimators\n",
    "\n",
    "* The covariance matrix of a Gaussian approximation to the likelihood defines a 1-sigma, 2D, elliptical, _frequentist confidence region_\n",
    "\n",
    "* Since this came from transforming the sampling distribution, which is a PDF over datasets, the confidence interval enables conclusions in terms of fractions of an ensemble of (hypothetical) datasets\n",
    "\n",
    "* The 68% confidence interval is the range of estimators that we expect to contain the true parameter value in 68% of the (hypothetical) datasets observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequentism and Bayesianism\n",
    "\n",
    "* In Frequentism, the data are considered to be random variables (in large sets of hypothetical trials described with probability distributions) while parameters are considered fixed (and to be estimated)\n",
    "\n",
    "* In Bayesianism, the data are considered to be fixed (as constants, in datafiles) while parameters are considered random variables (to be inferred, with uncertainty described by probability distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given an assumed model:\n",
    "\n",
    "* Frequentists seek to _transform_ the frequency distribution of the data into a frequency distribution of their estimators, and hence quantify their uncertainty in terms of _what they expect would happen if the observation were to be repeated_  \n",
    "  \n",
    "* Bayesians seek to _update their knowledge_ of their model parameters, and hence quantify their uncertainty in terms of _what might have been had the observation been different_, and _what they knew before the data were taken_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequentism vs. Bayesianism\n",
    "\n",
    "\n",
    "* \"You have to assume a prior\" cf. \"You get to assume a prior\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* \"Your calculations are computationally expensive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* \"How do you account for your nuisance parameters?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* \"Your conclusions are not relevant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Things to remember\n",
    "\n",
    "* The most important thing is to _know what you are doing_, and to _communicate that clearly to others_: both approaches involve assumptions which must be recorded and tested\n",
    "\n",
    "* The Bayesian approach provides a logical framework for combining datasets and additional information, and provides answers in terms of the probability distribution for the model parameters\n",
    "\n",
    "* The Frequentist approach provides a way of studying the model independent of additional information beyond the dataset in hand, and provides answers in terms of the probability of getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Endnote\n",
    "\n",
    "* The astronomy literature contains a mixture of frequentist and Bayesian analysis, sometimes within the same paper\n",
    "\n",
    "* Frequentist estimators often make good _summary statistics_ with well understood sampling distributions: astronomical catalogs are full of them\n",
    "\n",
    "* In most of this course we follow the Bayesian approach: Bayes' Theorem gives you a framework for deriving the solution to _any_ inference problem you encounter.  Having said that, we'll keep our eyes open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appendix: Computing the Bayesian Posterior PDF\n",
    "\n",
    "Here's how the figures for the Bayesian inference were made - starting with the functions for the log likelihood, the log prior, and the unnormalized log posterior, evaluated on a 2D $(a,b)$ parameter grid given the R11 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('graphics/cepheids.py').read())\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "\n",
    "data = Cepheids('../data/R11ceph.dat')\n",
    "\n",
    "def log_likelihood(logP, mobs, sigma, a, b):\n",
    "    return -0.5*np.sum(2*np.pi*sigma**2) - \\\n",
    "            0.5*np.sum((mobs - a*logP - b)**2/(sigma**2))\n",
    "\n",
    "def log_prior(a, b):\n",
    "\n",
    "    amin,amax = -10.0,10.0\n",
    "    bmin,bmax = 10.0,30.0\n",
    "\n",
    "    if (b > bmin)*(b < bmax):\n",
    "        value = 0.0\n",
    "# Interesting alterniative: Cauchy distribution for b, equivalent to uniform\n",
    "# in angle of orientation of line:\n",
    "#        value = np.log(1.0/(bmax-bmin)) - \\\n",
    "#                np.log(np.pi) - np.log(1 + a**2)\n",
    "    else:\n",
    "        value = -np.inf\n",
    "    \n",
    "    if (a > amin)*(a < amax):\n",
    "        value += 0.0\n",
    "    else:\n",
    "        value += -np.inf\n",
    "        \n",
    "    return value\n",
    "\n",
    "def log_posterior(logP, mobs, sigma, a, b):\n",
    "    return log_likelihood(logP,mobs,sigma,a,b) + log_prior(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the posterior PDF\n",
    "\n",
    "Now, let's set up a suitable parameter grid, evaluate the unnormalized log posterior on it, and then renormalize it numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limits of parameter grids, focused on the high likelihood region:\n",
    "amin, amax = -3.4, -2.4\n",
    "bmin, bmax = 25.7, 26.8\n",
    "limits = (amin, amax, bmin, bmax)\n",
    "\n",
    "def evaluate_posterior_on_a_grid(limits, NGC_ID=4258, npix=100):\n",
    "    \n",
    "    # Make grids:\n",
    "    amin, amax, bmin, bmax = limits\n",
    "    agrid, bgrid, logprob = np.linspace(amin,amax,npix), np.linspace(bmin,bmax,npix), np.zeros([npix,npix])\n",
    "\n",
    "    # Select a Cepheid dataset:\n",
    "    data.select(NGC_ID)\n",
    "\n",
    "    # Loop over parameters, computing unnormlized log posterior PDF:\n",
    "    for i,a in enumerate(agrid):\n",
    "        for j,b in enumerate(bgrid):\n",
    "            logprob[j,i] = log_posterior(data.logP, data.mobs, data.sigma, a, b)\n",
    "\n",
    "    # Exponentiate and normalize to get posterior density:\n",
    "    prob = np.exp(logprob - np.max(logprob))\n",
    "    prob /= np.sum(prob)\n",
    "    \n",
    "    return prob, agrid, bgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prob, a, b = evaluate_posterior_on_a_grid(limits, NGC_ID=4258, npix=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing the 2D PDF\n",
    "\n",
    "* Typically we want to be able to see the centroid, size and shape of the posterior PDF\n",
    "\n",
    "* In particular we want to see the _credible regions_ that enclose 68% and 95% of the posterior probability. These are best plotted as contours\n",
    "\n",
    "* Given our assumption that the model is true, the probability that the true values of the model parameters lie within the 95% credible region given the data is 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted = np.sort(prob.flatten())\n",
    "C = sorted.cumsum()\n",
    "\n",
    "# Find the pixel values that lie at the levels that contain 68% and 95% of the probability:\n",
    "lvl68 = np.min(sorted[C > (1.0 - 0.68)])\n",
    "lvl95 = np.min(sorted[C > (1.0 - 0.95)])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(prob, origin='lower', cmap='Blues', interpolation='none', extent=limits)\n",
    "plt.contour(prob,[lvl95,lvl68],colors='black',extent=limits)\n",
    "plt.grid()\n",
    "plt.xlabel('slope a', fontsize=20)\n",
    "plt.ylabel('intercept b / AB magnitudes', fontsize=20);\n",
    "\n",
    "# plt.savefig(\"cepheids_2d-posterior.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing our inferences\n",
    "\n",
    "Let's compute the 1D marginalized posterior PDFs for $a$ and for $b$, and report the median and \"68% credible interval\" (defined as the region of 1D parameter space enclosing 68% of the posterior probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_a_given_data = np.sum(prob, axis=0) # Approximate the integral as a sum\n",
    "prob_b_given_data = np.sum(prob, axis=1) # Approximate the integral as a sum\n",
    "\n",
    "# Check that we do have a 1D PDF:\n",
    "# print(prob_a_given_data.shape, np.sum(prob_a_given_data))\n",
    "\n",
    "plot_1d_marginalized_pdfs(a, b, prob_a_given_data, prob_b_given_data)\n",
    "\n",
    "# plt.savefig(\"cepheids_1d-posteriors.png\")\n",
    "\n",
    "print(\"a = \",compress_1D_pdf(a, prob_a_given_data, ci=68, dp=2))\n",
    "print(\"b = \",compress_1D_pdf(b, prob_b_given_data, ci=68, dp=2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Post-inference model checking\n",
    "\n",
    "\n",
    "Are these inferred parameters sensible? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(4258)\n",
    "\n",
    "data.overlay_straight_line_with(a=-2.95, b=26.25, label='Model')\n",
    "\n",
    "data.add_legend()\n",
    "\n",
    "# plt.savefig(\"cepheids_posterior-median-check.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appendix: Finding the Maximum likelihood parameters\n",
    "\n",
    "Here's the code to find the maximum likelihood parameters in the Cepheid problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "exec(open('graphics/cepheids.py').read())\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "\n",
    "data = Cepheids('../data/R11ceph.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select(4258)\n",
    "M, v = data.sufficient_statistics()\n",
    "a, b = np.linalg.solve(M, v)\n",
    "print('$ \\hat{a} = %.2f $' % np.round(a, 2))\n",
    "print('$ \\hat{b} = %.2f $' % np.round(b, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data.plot(4258)\n",
    "\n",
    "data.overlay_straight_line_with(a=a, b=b, label='Maximum Likelihood fit')\n",
    "\n",
    "data.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The covariance of MLEs\n",
    "\n",
    "Here's code to compute the inverse Hessian of the log likelihood, and hence a lower limit on the covariance of the ML estimators. The diagonal elements of the covariance matrix give an approximate symmetrical error bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Generalized maximum likelihood approach:\n",
    "\n",
    "import scipy.optimize\n",
    "\n",
    "pars = np.array([0.0, 20])\n",
    "result = scipy.optimize.minimize(data.negative_log_likelihood, pars, method='BFGS', tol=0.001)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = result.hess_inv\n",
    "np.sqrt(C[0,0]), np.sqrt(C[1,1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
