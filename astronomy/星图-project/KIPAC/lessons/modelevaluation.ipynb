{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating Models\n",
    "\n",
    "Goals:\n",
    "\n",
    "* Be able to design and carry out tests of model adequacy (goodness of fit), and comparisons between models\n",
    "\n",
    "* Understand and be prepared to use the Bayesian Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "* Gelman et al (BDA3), Chapter 6\n",
    "\n",
    "* Mackay, Chapters 3 and 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preamble\n",
    "\n",
    "You can't do inference without making assumptions.\n",
    "\n",
    "$\\longrightarrow$ We must _test_ the hypotheses defined by our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Three related but distinct questions come under the heading of **model evaluation**.\n",
    "1. Does a model describe (fit) the data well?\n",
    "2. Does a model make accurate predictions about new data?\n",
    "3. How probable are our competing models in light of the data?\n",
    "\n",
    "Often (2) and (3) are directly related to **model comparison** or **selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**A familiar example:** imagine we have a data set like this\n",
    "<img src=\"graphics/modelcheck-data.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Specifically,\n",
    "* we have precisely known $x$ values\n",
    "* we have precisely known, Gaussian errors on $y$\n",
    "* we're fitting a linear model, $\\bar{y}(x)=b+mx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this case, the likelihood is $\\propto e^{-\\chi^2/2}$.\n",
    "\n",
    "So is the posterior, given uniform priors on $m$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classical Hypothesis Testing\n",
    "\n",
    "Assuming this model (line plus Gaussian errors) is correct, the distribution over data sets of $\\hat{\\chi}^2$ must follow a $\\chi^2_\\nu$ distribution, where\n",
    "* $\\hat{\\chi}^2$ is the best-fit $\\chi^2$ over parameters for a given data set\n",
    "* the number of degrees of freedom $\\nu=N_\\mathrm{data}-N_\\mathrm{params}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hence, the classical $\\chi^2$ test looks at whether $\\hat{\\chi}^2$ is consistent with this distribution. If not, it's unlikely that our data came from the assumed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this case, the value of $\\hat{\\chi}^2\\approx104$ doesn't look good in light of the expectation.\n",
    "<img src=\"graphics/modelcheck-chisq.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The probability $P(\\chi^2\\geq\\hat{\\chi}^2|\\nu)$ ($\\sim10^{-10}$ in this case) is called the **$p$-value** or **significance**.\n",
    "\n",
    "* If the \"null hypothesis\" (our assumed model, with fitted parameters $[\\hat{m},\\hat{b}]$) is true, we expect the fraction of hypothetical new datasets to have $\\chi^2$ values greater than $\\hat{\\chi}^2$ to be $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The $p$-value is _not_ the probability of the model$(m,b)$ being true. Like the sampling distribution from which it is derived, it characterizes the probability of getting the data given the assumed model and its estimated parameters.\n",
    "\n",
    "The result of a classical hypothesis test is of the following form:\n",
    "\n",
    "_\"We reject the null hypothesis at the $p$ significance level\"_\n",
    "\n",
    "(i.e. on the grounds that it inadequately predicts the data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practical Chi-squared Testing\n",
    "\n",
    "* We can compute the p-value assuming a chi-squared distribution using `scipy.stats`:\n",
    "```python\n",
    "import scipy.stats\n",
    "chisq = scipy.stats.chi2(Ndof)\n",
    "pvalue = chisq.sf(chisq_min)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The \"reduced chi-squared\", $\\hat{\\chi}^2_{R} = \\hat{\\chi}^2 / N_{\\rm dof}$, is often used by astronomers to quantify goodness of fit - but note that you need to know the number of degrees of freedom separately from $\\hat{\\chi}^2$ to be able to interpret it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* A useful, quick way to make sense of  $\\hat{\\chi}^2$ and $N_{\\rm dof}$ values is to use **Fisher's Gaussian approximation to the chi-squared distribution**: \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\sqrt{2\\hat{\\chi}^2} \\sim \\mathcal{N}\\left( \\sqrt{2 N_{\\rm dof}-1}, 1 \\right)$  (approximately)\n",
    "\n",
    "$\\longrightarrow$ The difference between $\\sqrt{2\\hat{\\chi}^2}$ and $\\sqrt{2 N_{\\rm dof}-1}$ is the \"number of sigma\" ($n_{\\sigma}$) we are away from a good fit.\n",
    "\n",
    "> In our case, the MLE model is about 7-sigma away from being a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Hypothesis Testing\n",
    "\n",
    "* In general our likelihood won't have nice, analytic properties. \n",
    "\n",
    "* We will want to evaluate the success of our model at explaining the data taking our uncertainty in the model parameters into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can construct analogous hypothesis tests by _simulating many \"replica\" data sets realized from the posterior distribution,_ and\n",
    "\n",
    "comparing the observed data with the replica data via a suitable summary \"test statistic\", and its  **\"posterior predictive distribution\"**\n",
    "\n",
    "We are free to design our test statistic to focus on the aspect of the data that we want the model to fit well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Posterior predictive model checking** - logic:\n",
    "* If our model is the true one, then *replica* data generated by it should \"look like\" the one dataset we have. \n",
    "\n",
    "* This means that any *summary* $T$ of both the real dataset, $T(d)$, and the replica datasets, $T(d^{\\rm rep})$, should follow the same distribution over noise realizations _and_ model parameters.\n",
    "\n",
    "* If the real dataset was not generated with our model, then its summary may be an _outlier_ from the distribution of summaries of replica datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Note the similarity to the logic of the classical hypothesis test. The difference is that the Bayesian replica datasets were generated with plausible values of the parameters (drawn from the posterior PDF), while all the hypothetical datasets in frequentism (each with their own $\\hat{\\chi}^2$) are drawn using the same model parameters (the estimated ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visual comparison of _models_ drawn from the posterior with the data. (NB: there is no replica data here...)\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/modelcheck-linear-posterior.png\" width=90%></td>\n",
    "        <td></td>\n",
    "        <td><img src=\"graphics/modelcheck-linear.png\" width=90%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example test statistic: Pearson Correlation $r_{12}$\n",
    "\n",
    "* Focuses on the tightness of linear correlation between $x$ and $y$\n",
    "\n",
    "* $T(d) = r_{12} = \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\left[ \\sum_i (x_i - \\bar{x})^2 \\sum_i (y_i - \\bar{y})^2 \\right]^{1/2}}$\n",
    "\n",
    "For each one of many posterior samples, we draw a _replica dataset_ from the sampling distribution given the sample parameter vector, and compute $T(d^{\\rm rep})$, building up a histogram of $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "${\\rm P}[T(d^{\\rm rep})>T(d)\\,|\\,d] = 99.43\\%$ - our dataset $d$ is clearly an outlier.\n",
    "\n",
    "<img src=\"graphics/modelcheck-linear-TS.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The posterior predictive probability distribution for the test statistic $T(d)$ generated by sampling in this way is marginalized over both parameters and (replica) datasets \n",
    "\n",
    "* It takes into account both the uncertainty in the data (captured by the sampling distribution) _and_ the uncertainty in the parameters (propagated from our one dataset and our prior knowledge during posterior sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Posterior predictive model checking can be seen as the Bayesian extension of classical hypothesis testing, and is a useful test of _model adequacy_ \n",
    "\n",
    "* As with classical hypothesis testing, a model can be discarded (or retained) on the basis of a posterior predictive model check\n",
    "\n",
    "* Note that we did not have to make any approximations in order to use a standard distribution for our summary $T$: _we just used the posterior PDF we already had_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Test statistics $T(d,\\theta)$ that are functions of both the data and the parameters are called **discrepancy measures**.\n",
    "\n",
    "The maximum log-likelihood is a common example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Discrepancy measure: $T = \\hat{\\chi}^2$\n",
    "\n",
    "${\\rm Pr}(T(d^{\\rm rep},\\theta)>T(d,\\theta)\\,|\\,d) \\approx 0.0$\n",
    "\n",
    "<img src=\"graphics/modelcheck-linear-discrepancy.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Any way we look at it, it's unlikely that we'd conclude the linear model explains these data adequately. How do we choose an alternative?\n",
    "\n",
    "One way to compare the fitness of models is to look at question (2) in model evaluation: **How accurately do they predict new data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalized Predictive Accuracy\n",
    "\n",
    "* We typically want a fit that works well with any *potential* data set, rather than just reproducing the one we have.\n",
    "* In general, this means an \"Occam's Razor\"-like penalty for complexity should be involved (to avoid focusing on models that \"over-fit\" the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our example, we might add a quadratic term to the model: $y = b + m x + q x^2$. How do we quantify the improvement?\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"graphics/modelcheck-quadratic.png\" width=80%></td>\n",
    "<td><img src=\"graphics/modelcheck-quadratic-discrepancy.png\" width=80%></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The gold standard for testing predictive accuracy is to _get more data_.\n",
    "\n",
    "Short of that, the best option is **cross-validation**: fitting a model on many random subsets of the data and seeing how well it describes the complementary \"out of sample\" subsets.\n",
    "\n",
    "> This method is ubiquitous in machine learning, where accurate out-of-sample prediction is usually the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Short of exhaustive cross-validation, a number of **information criteria** exist that (asymptotically) relate to generalized predictive accuracy.\n",
    "\n",
    "These have the advantage of being relatively quick to calculate from the results of a fit - either an MLE or a set of posterior samples - and include a penalty for models with greater freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some information criteria:\n",
    "* Akaike information criterion (AIC)\n",
    "* Deviance information criterion (DIC)\n",
    "* Watanabe-Akaike information criterion (WAIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The DIC has the advantage of being compatible with Bayesian analysis (unlike AIC), and not requiring the data to be cleanly separable into conditionally independent subsets (unlike WAIC).\n",
    "\n",
    "$\\mathrm{DIC} = \\langle D(\\theta) \\rangle + p_D; \\quad p_D = \\langle D(\\theta) \\rangle - D(\\langle\\theta\\rangle)$\n",
    "\n",
    "where $D(\\theta)=-2\\log P(\\mathrm{data}|\\theta)$ and averages $\\langle\\rangle$ are over the posterior.\n",
    "\n",
    "$p_D$ is an _effective number of free parameters_, i.e. the number of parameters primarily constrained by the data rather than by their priors.\n",
    "\n",
    "The DIC thus doesn't necessarily count unconstrained nuisance parameters used to marginalize out systematics as \"added complexity\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that for all of these information criteria, a **lower** value is preferable (larger likelihood and/or less model complexity).\n",
    "\n",
    "A somewhat motivated scale for interpreting differences in IC exists (named for Jeffreys):\n",
    "\n",
    "| $e^{(\\mathrm{IC}_1-\\mathrm{IC}_2)/2}$  | Strength of evidence for model 2 |\n",
    "|--------|:------------------------:|\n",
    "|  < 1   | Negative |\n",
    "|  1-3   | Barely worth mentioning |\n",
    "|  3-10  | Substantial |\n",
    "| 10-30  | Strong |\n",
    "| 30-100 | Very strong |\n",
    "|   >100 | Decisive |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How about the third question - **How probable are our competing models in the light of the data?**\n",
    "\n",
    "* This question cannot be asked in classical statistics - where only data have probability distributions.\n",
    "\n",
    "* Bayes theorem gives us a framework for assessing relative model probabilities which naturally includes Occam's razor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Model Comparison\n",
    "\n",
    "Inference on parameters $\\theta$ given model $H$:\n",
    "\n",
    "$P(\\theta|D,H)=\\frac{P(D|\\theta,H)P(\\theta|H)}{P(D|H)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Inference on models $H$: \n",
    "\n",
    "$P(H|D,\\Omega)=\\frac{P(D|H,\\Omega)P(H|\\Omega)}{P(D|\\Omega)}$\n",
    "\n",
    "> NB. $H$ is a list of all of our assumptions - including our prior PDF assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here $\\Omega$ is some space of all allowed models. As we normally do for parameter inference, we'll work with a simplified version:\n",
    "\n",
    "$P(H|D)\\propto P(D|H)P(H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P(H)$ is a prior on the model, and\n",
    "\n",
    "$P(D|H)=\\int P(D|\\theta,H) \\, P(\\theta|H) d\\theta$ \n",
    "\n",
    "is the **evidence** - the normalizing denominator in Bayesian parameter inference (also known as the **fully marginalized likelihood**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ideally, we would compare models by looking at\n",
    "\n",
    "$\\frac{P(H_2|D)}{P(H_1|D)}=\\frac{P(D|H_2)\\,P(H_2)}{P(D|H_1)\\,P(H_1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "General difficulties in computing the terms in this ratio:\n",
    "* Assigning meaningful priors to models\n",
    "* Assigning meaningful priors to parameters\n",
    "* Calculating the evidence integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise: \"non-informative\" parameter priors and the evidence**\n",
    "\n",
    "Say we have a model with 1 parameter, $\\theta$, and a likelihood that works out to be a unit width Gaussian centered on $\\theta=0$ with peak value $\\mathcal{L}_{\\rm max}$.\n",
    "\n",
    "For each of the priors on $\\theta$ below, (a) sketch the likelihood and prior as a function of theta, (b) calculate the evidence for that model (well enough to comment about the relative differences, anyway).\n",
    "1. $P(\\theta|H_1)$ uniform on $[-1,+1]$\n",
    "2. $P(\\theta|H_2)$ uniform on $[-3,+3]$\n",
    "3. $P(\\theta|H_3)$ uniform on $[-100,+100]$\n",
    "4. $P(\\theta|H_4)$ uniform on $[-\\infty,+\\infty]$\n",
    "5. $P(\\theta|H_5)$ uniform on $[+3,+5]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"graphics/evidence.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Evidence exercise: notes**\n",
    "\n",
    "1) Models that are less prescriptive (in terms of their priors) are penalized in the evidence. This is a feature, although it means we need to put real thought into those priors.\n",
    "\n",
    "2) The evidence can be made arbitrarily small by increasing the prior volume: comparing evidences is more conservative than focusing on the goodness of fit ($L_{\\rm max}$) alone. \n",
    "\n",
    "3) The evidence is linearly sensitive to prior volume, but exponentially sensitive to goodness of fit ($L_{\\rm max} \\propto e^{-\\hat{\\chi}^2/2}$). It's still a likelihood, after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The evidence for model $H$, $P(D\\,|\\,H)$, enables a form of Bayesian hypothesis testing: model comparison with the \"evidence ratio\" or \"odds ratio\" or \"Bayes Factor\" $R$\n",
    "\n",
    "$R = \\frac{P(D|H_2)}{P(D|H_1)}$\n",
    "\n",
    "$R$ is a *fully marginalized likelihood ratio* - which is to say that it *takes into account our uncertainty about values of the parameters of each model by integrating over all plausible values of them.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that if your two models are equally probable _a priori_, then \n",
    "\n",
    "$\\frac{P(H_2)}{P(H_1)} = 1$ such that $\\frac{P(H_2|D)}{P(H_1|D)} = R$\n",
    "\n",
    "This assumption is often not always easy to justify, but it makes $R$ easy to interpret: its just the ratio of model probabilities in our ideal comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more practical way to interpret the Bayes factor is to note that it updates the model prior ratio into a posterior one. This means that:\n",
    "\n",
    "  * If you believe, despite having seen the data and computed $R$, that your two models are *still equally probable,*\n",
    "  \n",
    "  * then $R$ gives _the odds that you would have had to have been willing to take against $H_2$, before seeing the data._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our linear model fit example, we can compute the evidence for the linear and quadratic models, and form the odds ratio $R$.\n",
    "\n",
    "```\n",
    "log Evidence for Straight Line Model: -157.2\n",
    "log Evidence for Quadratic Model: -120.7\n",
    "Evidence ratio in favour of the Quadratic Model:\n",
    "  7e15 to 1\n",
    "```\n",
    "\n",
    "The 26 unit difference in log evidence between the two models translates to a _huge_ odds ratio in favour of the quadratic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Incidentally those data did not come from *either* a linear or quadratic model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The same Jeffreys scale used to interpret the information criteria can be used to interpret evidence ratios:\n",
    "\n",
    "| $R$    | Strength of Evidence for Model 2 |\n",
    "|--------|:------------------------:|\n",
    "|  < 1   | Negative |\n",
    "|  1-3   | Barely worth mentioning |\n",
    "|  3-10  | Substantial |\n",
    "| 10-30  | Strong |\n",
    "| 30-100 | Very strong |\n",
    "|   >100 | Decisive |\n",
    "\n",
    "> The Bayesian Information Criterion (BIC) is an approximation of $R$ (assuming $N$ datapoints greatly outnumber $k$ parameters, and the priors are uninformative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Calculating the evidence**\n",
    "\n",
    "Estimates directly calculated from Markov chains produced for parameter inference are generally not reliable.\n",
    "\n",
    "Good methods include nested sampling (e.g. [MultiNest](https://github.com/JohannesBuchner/PyMultiNest)) and parallel tempering / thermodynamc integration (e.g. [emcee](http://dan.iel.fm/emcee/current/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Bayesian Evidence: closing thoughts**\n",
    "\n",
    "* The Bayesian evidence is *qualitatively different* from other model assessments. While they focus primarily on *prediction accuracy,* the evidence is the way in which information from the prior PDF propagates through into our posterior beliefs about the model as a whole.\n",
    "\n",
    "* There are no inherent mathematical limitations to its use, in contrast to various other hypothesis tests that are only valid under certain assumptions (such as the models being nested, e.g. the classical $F$ test for comparing $\\chi^2$ values). _Any two models can be compared and the odds ratio computed._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Evaluation Summary\n",
    "\n",
    "1. Does a model describe (fit) the data well?\n",
    "> Posterior predictive model checks (visual, test stats, discrepancy measures)\n",
    "\n",
    "2. Does a model make accurate predictions about new data?\n",
    "> Cross validation; information criteria to quantify generalized predictive accuracy\n",
    "\n",
    "3. How probable are our competing models in light of the data?\n",
    "> Bayesian Evidence ratios (\"Bayes factors\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
