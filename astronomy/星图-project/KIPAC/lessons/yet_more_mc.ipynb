{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Even More Monte Carlo Sampling\n",
    "\n",
    "Goals:\n",
    "* Peruse the menu of sampling options beyond those that we've seen so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* Gelman ch. 12\n",
    "* Handbook of MCMC (Brooks, Gelman, Jones and Meng, eds.), parts of which are [on the web](http://www.mcmchandbook.net/HandbookTableofContents.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The wide world of sampling\n",
    "\n",
    "1. Evolving a single state\n",
    "  - Metropolis sampling, and related methods\n",
    "  - Hamiltonian Monte Carlo (HMC)\n",
    "2. Evolving an ensemble of states\n",
    "  - Goodman-Weare sampling\n",
    "3. Non-Markov methods\n",
    "  - Population Monte Carlo (PMC)\n",
    "  - Nested sampling\n",
    "  \n",
    "Codes for some of these are in our list of [MCMC packages](../doc/MCMC_packages.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hamiltonian Monte Carlo (HMC)\n",
    "\n",
    "*(Also sometimes called Hybrid Monte Carlo)*\n",
    "\n",
    "While standard MCMC is analogous to the evolution of a thermodynamic system, HMC is (almost) analogous to the evolution of a single particle. Consider our free parameters as coordinates of a position, $\\theta$, and minus the log-posterior as a potential energy, $U(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "HMC introduces momentum parameters, $\\phi$, corresponding to each position parameter, and an associated \"kinetic energy\",\n",
    "\n",
    "$K(\\phi) = \\sum_i \\frac{\\phi_i^2}{2m_i}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looking at the analogy the other way, the probability associated with $K(\\phi)$\n",
    "\n",
    "$p(\\phi) \\propto e^{-K(\\phi)}$\n",
    "\n",
    "is a multivariate Gaussian with mean zero and a diagonal covariance given by the \"masses\".\n",
    "\n",
    "The HMC algorithm alternates Gibbs samples of $\\phi|\\theta$ with joint updates of $\\phi$ *and* $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Generate a sample of $\\phi$ from the distribution defined by $K(\\phi)$.\n",
    "2. Evolve $(\\theta,\\phi)$ for some time as a dynamical system, according to $K(\\phi)$ and $U(\\theta)$.\n",
    "3. The state at the end of this process is the proposal. Apply the standard Metropolis acceptance test to the initial and proposed probabilities $e^{-(K+U)}$. (This is trivial if we conserve energy in the \"evolution\" phase, but in practice the evolution is often done more approximately to save cycles.)\n",
    "\n",
    "Note that this procedure requires *derivatives* of the log-posterior, unlike standard MCMC methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There's a lot more literature on HMC and its optimization than we can cover - see e.g. [this chapter](http://www.mcmchandbook.net/HandbookChapter5.pdf). In a nutshell, the introduction of \"momentum\" into the evolution of the chain is supposed to reduce the random-walk behavior (autocorrelation) of traditional MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What's the catch?\n",
    "\n",
    "> To evolve the state, we need the gradient of the log-posterior (the \"force\"). If this can be computed analytically, great. If we need to do it numerically, it isn't obvious that HMC will be more efficient than standard MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Affine-invariant sampling\n",
    "\n",
    "This is a class of methods that evolve an *ensemble* of states rather than a single state.\n",
    "After convergence, the ensemble can be regarded as a set of samples from the target distribution.\n",
    "\n",
    "This approach provides some of the benefits of running multiple chains - but remember that these are not *independent* chains!\n",
    "\n",
    "The currently fashionable variant is coded in a `python` package hilariously called `emcee`, and implements the evolution rule proposed by [Goodman and Weare (2010)](http://msp.org/camcos/2010/5-1/camcos-v5-n1-p04-p.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Goodman-Weare method\n",
    "\n",
    "The algorithm for moving each point in the ensemble is:\n",
    "1. Randomly pick a different point from the ensemble (total size $N$).\n",
    "2. Propose a move in the direction of that point, by the distance between them multiplied by a random from this distribution:\n",
    "$g(z) \\propto \\frac{1}{\\sqrt z}; ~ \\frac{1}{2}\\leq z \\leq 2$\n",
    "3. Accept or reject the move based on the ratio of posterior densities multiplied by $z^{N-1}$.\n",
    "\n",
    "Note that there is some magic in the density $g$. We are not free to choose just any function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This algorithm is relatively easy to use - there is no tuning required and it's straightforward to parallelize.\n",
    "\n",
    "Important cautions:\n",
    "* if the ensemble is not started in a region of high probability, convergence will be **extremely** slow. You have been warned.\n",
    "* as the walkers are not independent, the Gelman-Rubin convergence criterion doesn't apply\n",
    "* assessing convergence visually is not always straightforward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Population Monte Carlo (PMC)\n",
    "\n",
    "Recall the essence of MC integration: $\\int w(x)\\,p(x)\\,dx = \\int w(x)\\,dP(x) \\approx \\overline{w(x_i)}; ~ x_i\\sim P$. \n",
    "\n",
    "We used this (way back) to set up Simple Monte Carlo, where $p$ is the prior and $w$ is the likelihood.\n",
    "\n",
    "The key to SMC is that _any_ set of points can provide an unbiased estimate of an integral, as long as the weighting is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PMC is a more refined version of this - instead of sampling from the prior, we iteratively build up an approximation to the posterior distribution, called the generating distribution, along with a number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each iteration consists of\n",
    "1. Selecting the generating distribution, $q$, and\n",
    "2. Sampling a number of points from the generating distribution and computing the corresponding posterior densities, $\\pi$. These points can then be weighted by the ratio $\\pi/q$ and treated as samples of the posterior.\n",
    "\n",
    "Clearly, getting this to converge to the target quickly will require a clever selection of $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A nice feature of PMC is that the generating distribution can depend on the previous crops of points while still producing unbiased results, thanks to the explicit re-weighting.\n",
    "\n",
    "With a suitably intelligent implementation (e.g. see [this](http://www.jstor.org/stable/27594084)), the generating distribution can be evolved towards the posterior distribution, providing samples for a posteriori analysis as well as an estimate for the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested sampling\n",
    "\n",
    "This is a particular form of PMC that is primarily aimed at calculating the Bayesian evidence, the integral of the likelihood function over the prior, $\\int p(D|\\theta)\\,p(\\theta)d\\theta$.\n",
    "\n",
    "We begin with a large number of points sampled from the prior, and gradually evolve them towards higher-likelihood positions, keeping track of how the volume filled changes.\n",
    "\n",
    "The evidence can then be calculated as a numerical approximation of $\\int_0^1 L(V)\\,dV$ (assuming the prior volume is normalized to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By virtue of spamming the parameter space with points at the start, nested sampling is likely (though never guaranteed) to find multiple modes, if they exist (same for PMC).\n",
    "\n",
    "The computational challenge here is to maximize the efficiency of moving a given point to a position of higher likelihood. Math is involved - see e.g. the [`MultiNest` paper](http://arxiv.org/abs/0809.3437)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take-aways\n",
    "\n",
    "* We looked at a few (more) complicated sampling algorithms that are in use.\n",
    "* Each has pros and cons, in terms of generality, robustness, speed, interpretability, ...\n",
    "* There's no silver bullet in general - many problems can be solved perfectly well with simple methods, but sometimes you might be compelled to look farther, or to mix and match samplers."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
