{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Avoiding Fooling Ourselves: Detection, Fishing, Blinding, and Dataset Consistency\n",
    "\n",
    "Goals:\n",
    "\n",
    "* See how discoveries can be assessed in both the Bayesian and Frequentist frameworks\n",
    "\n",
    "* Understand the dangers of fishing expeditions\n",
    "\n",
    "* Understand unconscious experimenter bias, and how blinding can mitigate it\n",
    "\n",
    "* See how to quantify dataset consistency, when bringing multiple observations together\n",
    "\n",
    "_The first principle [of science] is that you must not fool yourself — and you are the easiest person to fool.”_ - Richard Feynman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further Reading\n",
    "\n",
    "* James Berger, [\"The Bayesian Approach to Discovery\"](https://indico.cern.ch/event/107747/contributions/32678/attachments/24371/35060/berger.pdf) (PHYSTAT 2011)\n",
    "\n",
    "* Kyle Cranmer, [\"Practical Statistics for the LHC\"](https://arxiv.org/pdf/1503.07622.pdf)\n",
    "\n",
    "* Gross & Vitells (2010), [\"Trial factors for the look elsewhere effect in high energy physics\"](https://arxiv.org/pdf/1005.1891.pdf)\n",
    "\n",
    "* MacCoun & Perlmutter (2015), [\"Hide Results to Seek the Truth\"](http://www.nature.com/polopoly_fs/1.18510!/menu/main/topColumns/topLeftColumn/pdf/526187a.pdf) \n",
    "\n",
    "* Klein & Roodman (2005) [\"Blind Analysis in Nuclear and Particle Physics\"](https://www.pp.rhul.ac.uk/~cowan/stat/annurev.nucl.55.090704.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"graphics/tour_cluster_spec_residuals.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detection\n",
    "\n",
    "* Source detection is different in character from parameter estimation. Initially, we are less interested in the properties of a new source than we are in its existence.\n",
    "\n",
    "\n",
    "* Detection is therefore a *model comparison* or *hypothesis testing* problem:\n",
    "\n",
    "  * $H_0$: the \"Null Hypothesis,\" that there is no source present\n",
    "  \n",
    "  * $H_1$: the \"Alternative Hypothesis,\" that there is a source present, e.g. with flux $f$ and position $(x,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Detection\n",
    "\n",
    "* We calculate and compare the Bayesian Evidences for each model, ${\\rm Pr}(d\\,|\\,H_0)$ and ${\\rm Pr}(d\\,|\\,H_1)$\n",
    "\n",
    "\n",
    "* Their ratio gives the relative probability of getting the data under the alternative and null hypotheses (and is equal to the relative probabilities of the hypotheses being true, up to an unknown model prior ratio).\n",
    "\n",
    "\n",
    "* This is an odds ratio (like \"ten to one\") that guides any bet we might want to make (staking, for example, some of our professional reputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prior Dependence\n",
    "\n",
    "* Calculating the Bayesian Evidence ratio involves marginalizing over the alternative hypothesis' model parameters, given the prior PDFs that we assigned when writing down $H_1$\n",
    "\n",
    "\n",
    "* Weakening the prior on the source position and flux (by, for example, expanding their ranges) makes _any given point in parameter space less probable_, the detected model correspondingly more contrived, and the data less likely\n",
    "\n",
    "\n",
    "* So, weaker priors _decrease_ the evidence for $H_1$, making the detection less significant (but only linearly, remember)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Frequentist Detection\n",
    "\n",
    "* Instead of working towards the relative probabilities of two hypotheses, the Frequentist approach is to attempt to reject the null hypothesis by showing that it would be too improbable for the data to have been generated by it\n",
    "\n",
    "\n",
    "* It turns out that the most _powerful_ statistic to use in this hypothesis test is the likelihood ratio\n",
    "\n",
    "\n",
    "> \"Power\" is defined to be the probability that the null hypothesis test is rejected when the alternative hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood Ratios\n",
    "\n",
    "* The procedure is to maximize the likelihood for the parameters of each hypothesis, and form the following test statistic:\n",
    "\n",
    "### $\\;\\;\\;\\;\\;\\;\\;T_d = 2 \\log \\frac{L(\\hat{f},\\,\\hat{x},\\,\\hat{y}\\,;\\,H_1)}{L(H_0)}$\n",
    "\n",
    "\n",
    "* We then inspect the distribution of test statistics $T$ over an ensemble of hypothetical datasets generated from a model with no source (the null hypothesis), and compute the $p$-value $P(T_d > T)$\n",
    "\n",
    "\n",
    "* If $p < \\alpha$ we then say that \"we can reject the null hypothesis at the $100\\,\\alpha$ percent confidence level.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Distribution of Test Statistics\n",
    "\n",
    "1) Simulation, in the \"Toy Monte Carlo\" approach: for a range of model parameters, generate large numbers of mock datasets, fit them, and compute $T$ for each one. Pros: reliable. Cons: CPU-intensive, depends on \"parameter ranges\"...\n",
    "\n",
    "\n",
    "2) Approximation: for large dataset size, $T \\sim \\chi^2(\\Delta\\nu)$, where $\\Delta\\nu$ is the difference in the number of degrees of freedom between the two hypotheses (3, in our example). This is [Wilks' Theorem](https://en.wikipedia.org/wiki/Likelihood-ratio_test#Distribution:_Wilks.E2.80.99_theorem)\n",
    "\n",
    "In case 2), we get a huge computational speed-up, but there's a catch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fishing Expeditions\n",
    "\n",
    "The test statistic depends only on the estimated values of the source parameters, $(\\hat{f},\\,\\hat{x},\\,\\hat{y})$, and reports the likelihood ratio between two _discrete_ hypotheses\n",
    "\n",
    "\n",
    "We need to account for the fact that we \"went fishing\" (ie, we _looked elsewhere_) for the source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"graphics/det_xkcd_1.png\">\n",
    "\n",
    "\n",
    "[XKCD \"Significant\"](https://xkcd.com/882), [CC NC-BY 2.5](https://creativecommons.org/licenses/by-nc/2.5/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"graphics/det_xkcd_2.png\" width=40%>\n",
    "\n",
    "\n",
    "[XKCD \"Significant\"](https://xkcd.com/882), [CC NC-BY 2.5](https://creativecommons.org/licenses/by-nc/2.5/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"graphics/det_xkcd_3.png\">\n",
    "\n",
    "\n",
    "[XKCD \"Significant\"](https://xkcd.com/882), [CC NC-BY 2.5](https://creativecommons.org/licenses/by-nc/2.5/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Bonferroni Correction\n",
    "\n",
    "If we carry out $m$ independent \"trials\", and are aiming to detect at the $\\alpha$ confidence level, we would expect to get a positive result by chance in a fraction $\\alpha' = 1 - (1-\\alpha)^m \\lesssim m \\alpha$ of cases\n",
    "\n",
    "\n",
    "Even if the trials are not independent, this last inequality still holds: the \"Bonferroni Correction\" involves comparing $p$-values to a threshold $\\alpha / m$, if you want to test (and report) at the $\\alpha$ confidence level. \n",
    "\n",
    "\n",
    "> This $1/m$ is sometimes referred to as the \"trials factor,\" and the issue described here is known in the statistics literature as the \"multiple comparisons\" problem, and in (astro)particle physics as the \"look elsewhere effect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question:\n",
    "How many discrete hypothesis tests are perfomed:\n",
    "\n",
    "* In the cartoon?\n",
    "\n",
    "* When detecting sources in an astronomical image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Another Question:\n",
    "\n",
    "* Does the simulation approach (1) suffer from the look elsewhere effect as well? \n",
    "\n",
    "* Does the Bayesian approach to detection suffer from the look elsewhere effect?\n",
    "\n",
    "Be prepared to give reasons..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Endnote: Classification and Detection\n",
    "\n",
    "Detection is similar to classification, in that we seek to assign labels to objects (\"detected\" or \"undetected\")\n",
    "\n",
    "\n",
    "Both involve a hypothesis test, and they share some terminology:\n",
    "\n",
    "* _False positives_ arise when the null hypothesis is incorrectly rejected: a \"Type I Error\"\n",
    "\n",
    "* _False negatives_ arise when the null hypothesis is incorrectly retained: a \"Type II Error\"\n",
    "\n",
    "\n",
    "The Bonferroni Correction serves to reduce Type I errors (at the risk of increasing the Type II error rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Post-Hoc Analysis\n",
    "\n",
    "Part of the problem in the cartoon was that more statistical tests were carried out _after the first one was reported._\n",
    "\n",
    "\n",
    "Such \"post-hoc\" (\"after the fact\") \"data dredging\" inflates the number of hypotheses, and care has to be taken.\n",
    "\n",
    "\n",
    "Post-hoc analysis is extremely common in astronomy, since our science is so often driven by new observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Avoiding Type I Errors\n",
    "\n",
    "\n",
    "Models suggested by an intial analysis of the data will naturally fit better. \n",
    "\n",
    "Allowing information from the same dataset to enter twice leads to the primary risk:\n",
    "\n",
    "\n",
    "* Generating false positives (in detection/classification) or \n",
    "* Under-estimating parameter uncertainties (in regression/measurement). \n",
    "\n",
    "\n",
    "The best way to avoid this problem is to _test the new models on new data_\n",
    "\n",
    "> This is the logic behind cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Analyses are Not Immune\n",
    "\n",
    "Recall that the Evidence is maximized by choosing a delta function prior centered on the maximum likelihood parameters\n",
    "\n",
    "\n",
    "The fact that Bayesian analysis necessarily involves writing down the prior PDF _is_ helpful: it should be clear that it's garbage coming out, if everyone can see that it's garbage going in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experimenter Bias\n",
    "\n",
    "A particularly insidious post-hoc analysis problem is \"unconscious experimenter bias:\"\n",
    "\n",
    "\n",
    "* After completing their analysis, a researcher compares their conclusion with those of others\n",
    "\n",
    "* If there is disagreement, the researcher is _more likely_ to try and \"fix the problem\" before publishing\n",
    "\n",
    "* Net result: there is a natural tendency towards \"concordance\" in the literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Blinding\n",
    "\n",
    "A practical solution to the problem of unconscious experimenter bias is to _blind_ the analysis\n",
    "\n",
    "\n",
    "* The analysis team is prevented from completing their inference and viewing the results until they deem their model complete (or at least, adequate)\n",
    "\n",
    "\n",
    "* The final inference is done once, and the results tagged as \"blinded\"\n",
    "\n",
    "\n",
    "* Post-hoc analysis is then enabled, but with results tagged as \"unblinded\" (since bias could be creeping in again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approaches to Blinding\n",
    "\n",
    "Before \"opening the box\" for the final inference run, one might use a method like:\n",
    "\n",
    "* **Hidden signal box**: the subset of data believed most likely to contain the signal is removed\n",
    "\n",
    "* **Hidden answer**: the numerical values of the parameters being measured are hidden, e.g. H0LiCOW (parameter means offset to zero), DES 3 yr (linear offsets added to correlation functions)\n",
    "\n",
    "* **Adding/removing data**: so that the team don't know whether they have detected a signal or not (\"salting\")\n",
    "\n",
    "* Training on a **\"pre-scaling\" subset**: as in cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mitigating against unconscious experimenter bias involves doing some qualitatively different things:\n",
    "\n",
    "\n",
    "* Organizing analyses in teams, and agreeing to abide by rules\n",
    "\n",
    "\n",
    "* Temporarily censoring or adjusting datasets while inferences are developed\n",
    "\n",
    "\n",
    "The results may not be pleasing, but blinding can increase confidence in the robustness of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples from Astrophysics & Cosmology\n",
    "\n",
    "Check out the talks given at the 2017 KIPAC Workshop, \"Blind Analysis in High-Stakes Survey Science: When, Why, and How?\": http://kipac.github.io/Blinding/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combining Datasets\n",
    "\n",
    "Blinding helps ensure that analysis of a model is not stopped too soon\n",
    "\n",
    "Systematic errors may still be present, and may not be revealed until multiple independent datasets are compared and found to be statistically inconsistent, or in _tension_, with each other.\n",
    "\n",
    "Lets look at a couple of ways to quantify the _consistency_ of two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The most common quantification of tension between two datasets is the distance between the central values of their likelihoods (or posterior PDFs), in units of \"sigma\"\n",
    "\n",
    "> \"sigma\" is usually taken to be the sum of the two posterior widths, in quadrature\n",
    "\n",
    "<img src=\"graphics/consistency.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An alternative quantification, that works in any number of dimensions, is to use the Bayesian evidence to quantify the overlap between likelihoods.\n",
    "\n",
    "Consider the following two models:\n",
    "\n",
    "* $H_1$: Both datasets $d_A$ and $d_B$ were generated from _the same global model_ with parameters $\\theta$. \n",
    "  * The evidence for $H_1$ is $P(\\{d_A,d_B\\}|H_1) \n",
    "    = \\int P(d_A|\\theta,H_1)P(d_B|\\theta,H_1)P(\\theta|H_1)\\,d\\theta$ \n",
    "\n",
    "This would be computed during a _joint fit_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $H_2$: Each dataset $d_A$ and $d_B$ was generated from _its own local model_ with parameters $\\theta_A$ and $\\theta_B$. \n",
    "  * The evidence for $H_2$ is $P(\\{d_A,d_B\\}|H_2)$ \n",
    "    \n",
    "    $= \\int P(d_A|\\theta_A,H_2)P(d_B|\\theta_B,H_2)P(\\theta_A|H_2)P(\\theta_B|H_2)\\,d\\theta_A d\\theta_B$ \n",
    "    \n",
    "    $= P(d_A|H_2)P(d_B|H_2)$\n",
    "    \n",
    "i.e. In $H_2$ the evidence is just the product of the evidences computed during the two separate fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Bayes factor is therefore\n",
    "\n",
    "$\\frac{P(\\{d_A,d_B\\}|H_2)}{P(\\{d_A,d_B\\}|H_1)} = \\frac{P(d_A|H_2)P(d_B|H_2)}{P(\\{d_A,d_B\\}|H_1)}$\n",
    "\n",
    "If the inferences of $\\theta_A$ and $\\theta_B$ under $H_2$ are very different, we would say that the datasets were inconsistent and the Bayes factor would be high: can you see why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The reduced goodness of fit in the joint model $H_1$ is balanced against the inclusion of a complete set of additional parameters in $H_2$. \n",
    "\n",
    "An unknown systematic error, causing model $H_1$ to fail to fit both datasets simultaneously, would lead to an unknown parameter vector offset - these additional parameters are present in $H_2$, which is better able to fit the data as a result.\n",
    "\n",
    "This Bayes factor approach has been used by the DES and H0LiCoW teams to quantify the consistency of their cosmological probes / lens datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Model evaluation (hypothesis testing) appears in a variety of situations.\n",
    "\n",
    "* Source detection and classification\n",
    "\n",
    "* Dataset combination\n",
    "\n",
    "* Systematic error modeling\n",
    "\n",
    "Blinding helps mitigate against unconscious experimenter bias when inferring parameters and evaluating models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appendix: Dataset Consistency Demo\n",
    "\n",
    "Below is the code used to make the dataset consistency illustration, in case its useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset consistency demo - the distance between two posterior means, in \"sigma\"\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Two model posterior PDFs for the same parameter given different datasets:\n",
    "mu1,sigma1 = 25,10\n",
    "mu2,sigma2 = 75,20\n",
    "\n",
    "# Add the sigmas in quadrature:\n",
    "sigma = np.sqrt(sigma1**2 + sigma2**2)\n",
    "# Compute the distance in units of this sigma:\n",
    "n = (mu2 - mu1)/sigma\n",
    "\n",
    "# Plot the distributions and annotate:\n",
    "x = np.linspace(0,100,100)\n",
    "\n",
    "p1 = scipy.stats.norm.pdf(x,loc=mu1,scale=sigma1)\n",
    "plt.plot(x,p1)\n",
    "yy = 0.6*max(p1)\n",
    "plt.annotate(s='', xy=(mu1,yy), xytext=(mu1+sigma1,yy), arrowprops=dict(arrowstyle='<->',lw=2))\n",
    "plt.text(mu1+0.5*sigma1,yy*0.9,'$\\sigma_1$',horizontalalignment='center',fontsize=14);\n",
    "\n",
    "p2 = scipy.stats.norm.pdf(x,loc=mu2,scale=sigma2)\n",
    "plt.plot(x,p2)\n",
    "yy = 0.6*max(p2)\n",
    "plt.annotate(s='', xy=(mu2,yy), xytext=(mu2+sigma2,yy), arrowprops=dict(arrowstyle='<->',lw=2))\n",
    "plt.text(mu2+0.5*sigma2,yy*0.8,'$\\sigma_2$',horizontalalignment='center',fontsize=14);\n",
    "\n",
    "plt.xlim(0,100)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Posterior probability P(x|data)\")\n",
    "yy = 0.75*max(max(p1),max(p2))\n",
    "plt.annotate(s='', xy=(mu1,yy), xytext=(mu2,yy), arrowprops=dict(arrowstyle='<->',lw=2))\n",
    "plt.text(0.5*(mu1+mu2),yy*1.1,'\"{:.1f} $\\sigma$\"'.format(n),horizontalalignment='center',fontsize=18);\n",
    "\n",
    "plt.text(0.52*(mu1+mu2),yy*0.63,'$\\sigma=\\sqrt{\\sigma_1^2+\\sigma_2^2}$',horizontalalignment='center',fontsize=14);\n",
    "\n",
    "plt.savefig(\"consistency.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
